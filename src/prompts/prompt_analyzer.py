#!/usr/bin/env python3
"""
Prompt Analyzer - –ê–Ω–∞–ª–∏–∑ failures –∏ implicit signals –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤

–°–æ–±–∏—Ä–∞–µ—Ç:
- Failures –∞–≥–µ–Ω—Ç–æ–≤ (task, error, context)
- Implicit signals (retry_count, edit_rate, execution_time)
- Explicit feedback (ratings –µ—Å–ª–∏ –µ—Å—Ç—å)

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç:
- Common patterns –≤ failures
- Correlation –º–µ–∂–¥—É –ø—Ä–æ–º–ø—Ç–æ–º –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
- Recommendations –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
"""

import os
import sys
import sqlite3
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from collections import Counter, defaultdict

PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.reporting.token_tracker import TokenTracker


class PromptAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - Failures –∏–∑ –ë–î
    - Implicit signals (retry, time)
    - Pattern analysis
    """

    def __init__(self, db_path: Optional[str] = None):
        """
        Args:
            db_path: –ü—É—Ç—å –∫ token_usage.db
        """
        self.db_path = db_path or os.path.join(PROJECT_ROOT, "data", "token_usage.db")
        self.tracker = TokenTracker(db_path=self.db_path)

    def collect_failures(self, agent_type: Optional[str] = None, days: int = 30) -> List[Dict]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç failures –∑–∞ –ø–µ—Ä–∏–æ–¥

        Args:
            agent_type: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∞–≥–µ–Ω—Ç–∞ (coder, reviewer, etc.)
            days: –ü–µ—Ä–∏–æ–¥ –≤ –¥–Ω—è—Ö

        Returns:
            –°–ø–∏—Å–æ–∫ failures —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        start_date = (datetime.now() - timedelta(days=days)).isoformat()

        # Failures = –∑–∞–¥–∞—á–∏ —Å success=0
        query = """
            SELECT
                task_id,
                query,
                complexity,
                started_at,
                finished_at
            FROM tasks
            WHERE success = 0
              AND started_at >= ?
        """
        params = [start_date]

        cursor.execute(query, params)
        rows = cursor.fetchall()

        failures = []
        for row in rows:
            task_id, description, complexity, timestamp, end_time = row

            # –ü–æ–ª—É—á–∞–µ–º LLM calls –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏
            cursor.execute("""
                SELECT model, role, input_tokens, output_tokens
                FROM llm_calls
                WHERE task_id = ?
                ORDER BY call_order
            """, (task_id,))

            llm_calls = cursor.fetchall()

            failures.append({
                'task_id': task_id,
                'description': description,
                'complexity': complexity,
                'timestamp': timestamp,
                'end_time': end_time,
                'llm_calls': [
                    {
                        'model': call[0],
                        'role': call[1],
                        'input_tokens': call[2],
                        'output_tokens': call[3]
                    }
                    for call in llm_calls
                ]
            })

        conn.close()
        return failures

    def collect_implicit_signals(self, days: int = 30) -> Dict:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç implicit signals (retry rate, execution time)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        start_date = (datetime.now() - timedelta(days=days)).isoformat()

        # –°—Ä–µ–¥–Ω–∏–π retry count (tasks —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º query)
        cursor.execute("""
            SELECT query, COUNT(*) as attempts
            FROM tasks
            WHERE started_at >= ?
            GROUP BY query
            HAVING COUNT(*) > 1
            ORDER BY attempts DESC
            LIMIT 20
        """, (start_date,))

        retry_patterns = [
            {'description': row[0], 'retry_count': row[1]}
            for row in cursor.fetchall()
        ]

        # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø–æ complexity
        cursor.execute("""
            SELECT
                complexity,
                AVG((julianday(finished_at) - julianday(started_at)) * 24 * 60) as avg_minutes
            FROM tasks
            WHERE started_at >= ? AND finished_at IS NOT NULL
            GROUP BY complexity
        """, (start_date,))

        avg_time_by_complexity = {
            row[0]: row[1] for row in cursor.fetchall()
        }

        # –ó–∞–¥–∞—á–∏ —Å –Ω–µ–æ–±—ã—á–Ω–æ –¥–æ–ª–≥–∏–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º (outliers)
        cursor.execute("""
            SELECT
                task_id,
                query,
                complexity,
                (julianday(finished_at) - julianday(started_at)) * 24 * 60 as minutes
            FROM tasks
            WHERE started_at >= ?
              AND finished_at IS NOT NULL
              AND minutes > (
                  SELECT AVG((julianday(finished_at) - julianday(started_at)) * 24 * 60) * 2
                  FROM tasks
                  WHERE complexity = tasks.complexity
              )
            ORDER BY minutes DESC
            LIMIT 10
        """, (start_date,))

        slow_tasks = [
            {
                'task_id': row[0],
                'description': row[1],
                'complexity': row[2],
                'minutes': row[3]
            }
            for row in cursor.fetchall()
        ]

        conn.close()

        return {
            'retry_patterns': retry_patterns,
            'avg_time_by_complexity': avg_time_by_complexity,
            'slow_tasks': slow_tasks
        }

    def analyze_patterns(self, failures: List[Dict]) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç patterns –≤ failures

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
        """
        if not failures:
            return {
                'common_keywords': [],
                'complexity_distribution': {},
                'model_failure_rates': {},
                'recommendations': [],
                'total_failures': 0
            }

        # –ò–∑–≤–ª–µ–∫–∞–µ–º keywords –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π failures
        keywords = []
        for failure in failures:
            desc = failure['description'].lower()
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å NLP)
            words = desc.split()
            keywords.extend([w for w in words if len(w) > 3])

        common_keywords = Counter(keywords).most_common(10)

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ complexity
        complexity_dist = Counter([f['complexity'] for f in failures])

        # Failure rate –ø–æ –º–æ–¥–µ–ª–∏ (–∏–∑ LLM calls)
        model_calls = defaultdict(int)
        for failure in failures:
            for call in failure['llm_calls']:
                model_calls[call['model']] += 1

        return {
            'common_keywords': common_keywords,
            'complexity_distribution': dict(complexity_dist),
            'model_usage_in_failures': dict(model_calls),
            'total_failures': len(failures)
        }

    def generate_recommendations(
        self,
        failures: List[Dict],
        patterns: Dict,
        implicit_signals: Dict
    ) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        recommendations = []

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1: –ï—Å–ª–∏ –º–Ω–æ–≥–æ failures –¥–ª—è complex –∑–∞–¥–∞—á
        if patterns['complexity_distribution'].get('complex', 0) > len(failures) * 0.5:
            recommendations.append(
                "üìã –ú–Ω–æ–≥–æ failures –¥–ª—è complex –∑–∞–¥–∞—á. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: "
                "–£–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á - –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, "
                "examples, –∏ step-by-step instructions."
            )

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2: –ï—Å–ª–∏ –≤—ã—Å–æ–∫–∏–π retry rate
        if implicit_signals['retry_patterns']:
            top_retry = implicit_signals['retry_patterns'][0]
            recommendations.append(
                f"üîÑ –í—ã—Å–æ–∫–∏–π retry rate –¥–ª—è –∑–∞–¥–∞—á —Ç–∏–ø–∞ '{top_retry['description'][:50]}...'. "
                "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –£—Ç–æ—á–Ω–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á, –¥–æ–±–∞–≤–∏—Ç—å constraints."
            )

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 3: –ï—Å–ª–∏ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ
        if implicit_signals['slow_tasks']:
            recommendations.append(
                "‚è±Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—á–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –Ω–µ–æ–±—ã—á–Ω–æ –¥–æ–ª–≥–æ. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: "
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ —Ä–∞–∑–±–∏—Ç—å –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏."
            )

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 4: –ï—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —á–∞—Å—Ç–æ –≤ failures
        if patterns['model_usage_in_failures']:
            most_used_model = max(
                patterns['model_usage_in_failures'].items(),
                key=lambda x: x[1]
            )[0]
            recommendations.append(
                f"ü§ñ –ú–æ–¥–µ–ª—å '{most_used_model}' —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ failures. "
                "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –í–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–µ–Ω –±–æ–ª–µ–µ –º–æ—â–Ω—ã–π model –¥–ª—è —ç—Ç–∏—Ö –∑–∞–¥–∞—á, "
                "–∏–ª–∏ –ø—Ä–æ–º–ø—Ç –Ω–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ —ç—Ç—É –º–æ–¥–µ–ª—å."
            )

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 5: Common keywords –≤ failures
        if patterns['common_keywords']:
            top_keywords = [kw[0] for kw in patterns['common_keywords'][:3]]
            recommendations.append(
                f"üîç –ß–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –≤ failures: {', '.join(top_keywords)}. "
                "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –î–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∑–∞–¥–∞—á —Å —ç—Ç–∏–º–∏ keywords."
            )

        if not recommendations:
            recommendations.append("‚úÖ –ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã—è–≤–∏–ª –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º. –ü—Ä–æ–º–ø—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ.")

        return recommendations

    def analyze_agent_type(self, agent_type: str, days: int = 30) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –∞–≥–µ–Ω—Ç–∞

        Args:
            agent_type: –¢–∏–ø –∞–≥–µ–Ω—Ç–∞ (coder, reviewer, etc.)
            days: –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        failures = self.collect_failures(agent_type=agent_type, days=days)
        implicit_signals = self.collect_implicit_signals(days=days)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        patterns = self.analyze_patterns(failures)
        recommendations = self.generate_recommendations(failures, patterns, implicit_signals)

        return {
            'agent_type': agent_type,
            'period_days': days,
            'analysis_timestamp': datetime.now().isoformat(),
            'failures': {
                'total': len(failures),
                'recent_examples': failures[:5]  # –¢–æ–ø-5 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
            },
            'patterns': patterns,
            'implicit_signals': implicit_signals,
            'recommendations': recommendations
        }

    def print_analysis(self, analysis: Dict) -> None:
        """–í—ã–≤–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –≤ CLI"""
        print("\n" + "="*80)
        print(f"  üìä –ê–ù–ê–õ–ò–ó –ü–†–û–ú–ü–¢–û–í - {analysis['agent_type']}")
        print(f"  –ü–µ—Ä–∏–æ–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {analysis['period_days']} –¥–Ω–µ–π")
        print("="*80 + "\n")

        print(f"Failures: {analysis['failures']['total']}")
        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:")
        for complexity, count in analysis['patterns']['complexity_distribution'].items():
            print(f"  {complexity}: {count}")

        print(f"\n–û–±—â–∏–µ keywords –≤ failures:")
        for keyword, count in analysis['patterns']['common_keywords'][:5]:
            print(f"  {keyword}: {count} —Ä–∞–∑")

        print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\n")
        for i, rec in enumerate(analysis['recommendations'], 1):
            print(f"{i}. {rec}\n")

        print("="*80 + "\n")


def main():
    """CLI entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
    parser.add_argument('--agent-type', type=str, help='–¢–∏–ø –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--days', type=int, default=30, help='–ü–µ—Ä–∏–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–¥–Ω–µ–π)')
    parser.add_argument('--export', type=str, help='–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ JSON')
    parser.add_argument('--db', type=str, help='–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö')

    args = parser.parse_args()

    analyzer = PromptAnalyzer(db_path=args.db)

    if args.agent_type:
        analysis = analyzer.analyze_agent_type(args.agent_type, days=args.days)
        analyzer.print_analysis(analysis)

        if args.export:
            with open(args.export, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, indent=2, ensure_ascii=False)
            print(f"–ê–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {args.export}")
    else:
        # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –∞–≥–µ–Ω—Ç—É
        analysis = analyzer.analyze_agent_type('all', days=args.days)
        analyzer.print_analysis(analysis)


if __name__ == "__main__":
    main()
