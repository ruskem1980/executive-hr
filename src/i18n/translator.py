#!/usr/bin/env python3
"""
Translator - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å—Ç—Ä–æ–∫ —á–µ—Ä–µ–∑ LLM.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¢–û–õ–¨–ö–û –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞.
–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å—Ö–æ–¥ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑:
- –ë–∞—Ç—á–∏–Ω–≥ —Å—Ç—Ä–æ–∫ (–¥–æ 50 –∑–∞ –∑–∞–ø—Ä–æ—Å)
- –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —É–∂–µ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –±–µ–∑ LLM
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è LLM (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —Ñ–∞–π–ª, –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã)

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ backend'—ã:
- Gemini (—á–µ—Ä–µ–∑ gemini-bridge.sh) - –¥–µ—à—ë–≤—ã–π
- OpenAI API (—á–µ—Ä–µ–∑ requests)
- Anthropic API (—á–µ—Ä–µ–∑ requests)
- Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–π)
"""

import json
import re
import subprocess
import time
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

from .catalog import TranslationCatalog, TranslationEntry
from .scanner import detect_language


# –°–ª–æ–≤–∞—Ä—å —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤ (–±–µ–∑ LLM)
TRIVIAL_TRANSLATIONS = {
    "ru_to_en": {
        # –£—Ä–æ–≤–Ω–∏ –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç–∏
        "CRITICAL": "CRITICAL", "HIGH": "HIGH", "MEDIUM": "MEDIUM", "LOW": "LOW",
        # –°—Ç–∞—Ç—É—Å—ã
        "completed": "completed", "failed": "failed", "error": "error",
        "passed": "passed", "skipped": "skipped", "timeout": "timeout",
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ
        "unknown": "unknown", "none": "none", "N/A": "N/A",
    },
    "en_to_ru": {
        "CRITICAL": "–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô",
        "HIGH": "–í–´–°–û–ö–ò–ô",
        "MEDIUM": "–°–†–ï–î–ù–ò–ô",
        "LOW": "–ù–ò–ó–ö–ò–ô",
        "completed": "–∑–∞–≤–µ—Ä—à–µ–Ω–æ",
        "failed": "–ø—Ä–æ–≤–∞–ª–µ–Ω–æ",
        "error": "–æ—à–∏–±–∫–∞",
        "passed": "–ø—Ä–æ–π–¥–µ–Ω–æ",
        "skipped": "–ø—Ä–æ–ø—É—â–µ–Ω–æ",
        "timeout": "—Ç–∞–π–º-–∞—É—Ç",
        "unknown": "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
        "Found": "–ù–∞–π–¥–µ–Ω–æ",
        "issues": "–ø—Ä–æ–±–ª–µ–º",
        "including": "–≤–∫–ª—é—á–∞—è",
        "Address CRITICAL issues immediately": "–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ —É—Å—Ç—Ä–∞–Ω–∏—Ç–µ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ø—Ä–æ–±–ª–µ–º—ã",
        "Consider incremental refactoring": "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥",
        "Run security audit before deployment": "–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞—É–¥–∏—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ —Ä–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ–º",
        "review code": "–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–¥",
        "use env vars": "–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è",
        "use secrets module": "–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥—É–ª—å secrets",
        "validate SSL certificates": "–ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç—ã",
        "parameterize queries": "–ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑—É–π—Ç–µ –∑–∞–ø—Ä–æ—Å—ã",
        "Reduce cyclomatic complexity in identified functions":
            "–°–Ω–∏–∑—å—Ç–µ —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö",
        "Consider caching expensive operations":
            "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π",
        "Profile runtime with py-spy or cProfile":
            "–ü—Ä–æ—Ñ–∏–ª–∏—Ä—É–π—Ç–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é py-spy –∏–ª–∏ cProfile",
        "Focus on CRITICAL priority files first":
            "–°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ñ–∞–π–ª—ã —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º CRITICAL",
        "Add unit tests for uncovered code paths":
            "–î–æ–±–∞–≤—å—Ç–µ —é–Ω–∏—Ç-—Ç–µ—Å—Ç—ã –¥–ª—è –Ω–µ–ø–æ–∫—Ä—ã—Ç—ã—Ö –ø—É—Ç–µ–π –∫–æ–¥–∞",
        "Review integration test coverage":
            "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Ç–µ—Å—Ç–∞–º–∏",
    }
}


@dataclass
class TranslationConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞."""
    backend: str = "gemini"         # gemini | openai | anthropic | ollama
    model: str = "flash"            # flash | pro | gpt-4o-mini | etc.
    batch_size: int = 30            # –°—Ç—Ä–æ–∫ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ LLM
    rate_limit_delay: float = 1.0   # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
    max_retries: int = 3            # –ú–∞–∫—Å. –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    temperature: float = 0.3        # –ù–∏–∑–∫–∞—è = –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥
    gemini_bridge_path: str = ""    # –ü—É—Ç—å –∫ gemini-bridge.sh
    api_key: str = ""               # API –∫–ª—é—á (–¥–ª—è openai/anthropic)
    ollama_url: str = "http://localhost:11434"  # URL Ollama


class LLMTranslator:
    """
    –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Å—Ç—Ä–æ–∫ —á–µ—Ä–µ–∑ LLM.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LLM —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫,
    –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—è —Ä–∞—Å—Ö–æ–¥ —Ç–æ–∫–µ–Ω–æ–≤ —á–µ—Ä–µ–∑ –±–∞—Ç—á–∏–Ω–≥ –∏ —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏.
    """

    def __init__(self, config: Optional[TranslationConfig] = None):
        self.config = config or TranslationConfig()
        self._setup_backend()

    def _setup_backend(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç backend –¥–ª—è LLM –≤—ã–∑–æ–≤–æ–≤."""
        if self.config.backend == "gemini":
            if not self.config.gemini_bridge_path:
                self.config.gemini_bridge_path = str(
                    Path(__file__).parent.parent.parent / ".claude" / "helpers" / "gemini-bridge.sh"
                )
        elif self.config.backend == "openai":
            if not self.config.api_key:
                self.config.api_key = os.environ.get("OPENAI_API_KEY", "")
        elif self.config.backend == "anthropic":
            if not self.config.api_key:
                self.config.api_key = os.environ.get("ANTHROPIC_API_KEY", "")

    def translate_catalog(self, catalog: TranslationCatalog,
                          source_locale: str, target_locale: str,
                          force: bool = False) -> Dict[str, int]:
        """
        –ü–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—Å–µ –Ω–µ–ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∫–∞—Ç–∞–ª–æ–≥–∞.

        Args:
            catalog: –ö–∞—Ç–∞–ª–æ–≥ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
            source_locale: –ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫ (ru, en)
            target_locale: –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫ (en, ru, de, etc.)
            force: –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã

        Returns:
            Dict —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π: trivial, llm, errors, skipped
        """
        entries = catalog.load_full(target_locale)
        if not entries:
            # –ï—Å–ª–∏ –∫–∞—Ç–∞–ª–æ–≥ —Ü–µ–ª–µ–≤–æ–≥–æ —è–∑—ã–∫–∞ –ø—É—Å—Ç, –∫–æ–ø–∏—Ä—É–µ–º –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ
            source_entries = catalog.load_full(source_locale)
            entries = {
                k: TranslationEntry(
                    original=v.original,
                    translated="",
                    category=v.category,
                    source_file=v.source_file,
                    source_line=v.source_line,
                    status="pending",
                    context=v.context,
                    has_placeholders=v.has_placeholders,
                )
                for k, v in source_entries.items()
            }

        stats = {"trivial": 0, "llm": 0, "errors": 0, "skipped": 0, "total": len(entries)}

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–µ –∏ —Ç—Ä–µ–±—É—é—â–∏–µ LLM
        trivial_entries = []
        llm_entries = []

        direction = f"{source_locale}_to_{target_locale}"
        trivial_dict = TRIVIAL_TRANSLATIONS.get(direction, {})

        for key, entry in entries.items():
            if entry.translated and not force:
                stats["skipped"] += 1
                continue

            if key in trivial_dict:
                entry.translated = trivial_dict[key]
                entry.status = "translated"
                entry.translator = "trivial_dict"
                trivial_entries.append(entry)
                stats["trivial"] += 1
            else:
                llm_entries.append(entry)

        # –ë–∞—Ç—á–∏–º LLM –ø–µ—Ä–µ–≤–æ–¥—ã
        if llm_entries:
            print(f"\n  –ü–µ—Ä–µ–≤–æ–¥ {len(llm_entries)} —Å—Ç—Ä–æ–∫ —á–µ—Ä–µ–∑ LLM ({self.config.backend})...")
            batches = self._create_batches(llm_entries, self.config.batch_size)

            for i, batch in enumerate(batches):
                print(f"    –ë–∞—Ç—á {i+1}/{len(batches)} ({len(batch)} —Å—Ç—Ä–æ–∫)...")
                try:
                    translations = self._translate_batch(
                        batch, source_locale, target_locale
                    )
                    for entry, translation in zip(batch, translations):
                        if translation:
                            entry.translated = translation
                            entry.status = "translated"
                            entry.translator = f"llm:{self.config.backend}"
                            stats["llm"] += 1
                        else:
                            stats["errors"] += 1

                    # Rate limiting
                    if i < len(batches) - 1:
                        time.sleep(self.config.rate_limit_delay)

                except Exception as e:
                    print(f"    –û—à–∏–±–∫–∞ –±–∞—Ç—á–∞ {i+1}: {e}")
                    stats["errors"] += len(batch)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        catalog.save(target_locale, entries)

        return stats

    def translate_single(self, text: str, source_locale: str,
                         target_locale: str, context: str = "") -> str:
        """
        –ü–µ—Ä–µ–≤–æ–¥–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É —á–µ—Ä–µ–∑ LLM.

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            source_locale: –Ø–∑—ã–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
            target_locale: –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞

        Returns:
            –ü–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
        direction = f"{source_locale}_to_{target_locale}"
        trivial = TRIVIAL_TRANSLATIONS.get(direction, {})
        if text in trivial:
            return trivial[text]

        # LLM –ø–µ—Ä–µ–≤–æ–¥
        prompt = self._build_single_prompt(text, source_locale, target_locale, context)
        response = self._call_llm(prompt)
        return self._extract_translation(response, text)

    def _translate_batch(self, entries: List[TranslationEntry],
                         source_locale: str, target_locale: str) -> List[str]:
        """–ü–µ—Ä–µ–≤–æ–¥–∏—Ç –±–∞—Ç—á —Å—Ç—Ä–æ–∫ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω –≤—ã–∑–æ–≤ LLM."""
        prompt = self._build_batch_prompt(entries, source_locale, target_locale)
        response = self._call_llm(prompt)
        return self._parse_batch_response(response, len(entries))

    def _build_batch_prompt(self, entries: List[TranslationEntry],
                            source_locale: str, target_locale: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –±–∞—Ç—á–µ–≤–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞."""
        locale_names = {
            "ru": "—Ä—É—Å—Å–∫–∏–π", "en": "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π", "de": "–Ω–µ–º–µ—Ü–∫–∏–π",
            "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π", "es": "–∏—Å–ø–∞–Ω—Å–∫–∏–π", "zh": "–∫–∏—Ç–∞–π—Å–∫–∏–π",
            "ja": "—è–ø–æ–Ω—Å–∫–∏–π", "ko": "–∫–æ—Ä–µ–π—Å–∫–∏–π", "pt": "–ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏–π",
            "it": "–∏—Ç–∞–ª—å—è–Ω—Å–∫–∏–π", "ar": "–∞—Ä–∞–±—Å–∫–∏–π", "hi": "—Ö–∏–Ω–¥–∏",
        }

        source_name = locale_names.get(source_locale, source_locale)
        target_name = locale_names.get(target_locale, target_locale)

        lines = []
        for i, entry in enumerate(entries, 1):
            ctx = ""
            if entry.context:
                ctx = f" [–∫–æ–Ω—Ç–µ–∫—Å—Ç: {entry.context}]"
            if entry.has_placeholders:
                ctx += " [–°–û–•–†–ê–ù–ò –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã {{...}} –∫–∞–∫ –µ—Å—Ç—å!]"
            lines.append(f"{i}. {entry.original}{ctx}")

        strings_block = "\n".join(lines)

        prompt = f"""–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ IT-–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤.
–ü–µ—Ä–µ–≤–µ–¥–∏ —Å—Ç—Ä–æ–∫–∏ —Å {source_name} –Ω–∞ {target_name}.

–ü–†–ê–í–ò–õ–ê:
1. –°–æ—Ö—Ä–∞–Ω—è–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∫–∞–∫ –µ—Å—Ç—å (API, SQL, JSON, pytest, bandit –∏ —Ç.–¥.)
2. –°–æ—Ö—Ä–∞–Ω—è–π –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã {{...}}, %s, {{name}} –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
3. –°–æ—Ö—Ä–∞–Ω—è–π —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã (‚úÖ ‚ùå ‚ö†Ô∏è üî¥ üü° üü¢)
4. –°–æ—Ö—Ä–∞–Ω—è–π —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫, –æ—Ç—Å—Ç—É–ø—ã)
5. –ü–µ—Ä–µ–≤–æ–¥–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–µ –¥–æ—Å–ª–æ–≤–Ω–æ
6. –î–ª—è IT-—Ç–µ—Ä–º–∏–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∏–Ω—è—Ç—ã–µ –≤ –æ—Ç—Ä–∞—Å–ª–∏ –ø–µ—Ä–µ–≤–æ–¥—ã

–°–¢–†–û–ö–ò –î–õ–Ø –ü–ï–†–ï–í–û–î–ê:
{strings_block}

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫, –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.
–ü—Ä–∏–º–µ—Ä: ["–ø–µ—Ä–µ–≤–æ–¥ 1", "–ø–µ—Ä–µ–≤–æ–¥ 2", "–ø–µ—Ä–µ–≤–æ–¥ 3"]

–í–µ—Ä–Ω–∏ –†–û–í–ù–û {len(entries)} –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ."""

        return prompt

    def _build_single_prompt(self, text: str, source_locale: str,
                             target_locale: str, context: str = "") -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
        locale_names = {
            "ru": "—Ä—É—Å—Å–∫–∏–π", "en": "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π", "de": "–Ω–µ–º–µ—Ü–∫–∏–π",
            "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π", "es": "–∏—Å–ø–∞–Ω—Å–∫–∏–π",
        }
        source_name = locale_names.get(source_locale, source_locale)
        target_name = locale_names.get(target_locale, target_locale)

        ctx_note = f"\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}" if context else ""

        return f"""–ü–µ—Ä–µ–≤–µ–¥–∏ —Å {source_name} –Ω–∞ {target_name}.
–°–æ—Ö—Ä–∞–Ω—è–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã, –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –∏ —ç–º–æ–¥–∑–∏.{ctx_note}

–¢–µ–∫—Å—Ç: {text}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –ø–µ—Ä–µ–≤–æ–¥, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."""

    def _call_llm(self, prompt: str) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç LLM backend."""
        for attempt in range(self.config.max_retries):
            try:
                if self.config.backend == "gemini":
                    return self._call_gemini(prompt)
                elif self.config.backend == "openai":
                    return self._call_openai(prompt)
                elif self.config.backend == "anthropic":
                    return self._call_anthropic(prompt)
                elif self.config.backend == "ollama":
                    return self._call_ollama(prompt)
                else:
                    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π backend: {self.config.backend}")
            except Exception as e:
                if attempt < self.config.max_retries - 1:
                    wait = (attempt + 1) * 2
                    print(f"    –û—à–∏–±–∫–∞ LLM (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}): {e}. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait}—Å...")
                    time.sleep(wait)
                else:
                    raise

    def _call_gemini(self, prompt: str) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç Gemini —á–µ—Ä–µ–∑ bridge —Å–∫—Ä–∏–ø—Ç."""
        bridge_path = self.config.gemini_bridge_path

        if not Path(bridge_path).exists():
            # Fallback –Ω–∞ –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤ gemini CLI
            try:
                result = subprocess.run(
                    ["gemini", "-m", f"gemini-2.5-{self.config.model}", "-p", prompt],
                    capture_output=True, text=True, timeout=120
                )
                if result.returncode == 0:
                    return result.stdout.strip()
                raise Exception(f"Gemini CLI error: {result.stderr}")
            except FileNotFoundError:
                raise Exception(
                    "Gemini CLI –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install google-genai"
                )

        result = subprocess.run(
            ["bash", bridge_path, self.config.model, prompt],
            capture_output=True, text=True, timeout=120
        )
        if result.returncode == 0:
            return result.stdout.strip()
        raise Exception(f"Gemini bridge error: {result.stderr}")

    def _call_openai(self, prompt: str) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç OpenAI API."""
        import urllib.request

        model = self.config.model or "gpt-4o-mini"
        data = json.dumps({
            "model": model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.config.temperature,
        }).encode()

        req = urllib.request.Request(
            "https://api.openai.com/v1/chat/completions",
            data=data,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.config.api_key}",
            }
        )

        with urllib.request.urlopen(req, timeout=120) as resp:
            result = json.loads(resp.read())
            return result["choices"][0]["message"]["content"].strip()

    def _call_anthropic(self, prompt: str) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç Anthropic API."""
        import urllib.request

        model = self.config.model or "claude-haiku-4-5-20251001"
        data = json.dumps({
            "model": model,
            "max_tokens": 4096,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.config.temperature,
        }).encode()

        req = urllib.request.Request(
            "https://api.anthropic.com/v1/messages",
            data=data,
            headers={
                "Content-Type": "application/json",
                "x-api-key": self.config.api_key,
                "anthropic-version": "2023-06-01",
            }
        )

        with urllib.request.urlopen(req, timeout=120) as resp:
            result = json.loads(resp.read())
            return result["content"][0]["text"].strip()

    def _call_ollama(self, prompt: str) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç Ollama (–ª–æ–∫–∞–ª—å–Ω—ã–π LLM)."""
        import urllib.request

        model = self.config.model or "llama3"
        data = json.dumps({
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": self.config.temperature},
        }).encode()

        req = urllib.request.Request(
            f"{self.config.ollama_url}/api/generate",
            data=data,
            headers={"Content-Type": "application/json"},
        )

        with urllib.request.urlopen(req, timeout=300) as resp:
            result = json.loads(resp.read())
            return result["response"].strip()

    def _parse_batch_response(self, response: str, expected_count: int) -> List[str]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –±–∞—Ç—á–µ–≤–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞."""
        # –£–±–∏—Ä–∞–µ–º markdown –±–ª–æ–∫–∏
        cleaned = response.strip()
        if cleaned.startswith("```"):
            cleaned = re.sub(r'```(?:json)?\n?', '', cleaned).strip()

        try:
            translations = json.loads(cleaned)
            if isinstance(translations, list):
                # –î–æ–±–∏–≤–∞–µ–º –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏
                while len(translations) < expected_count:
                    translations.append("")
                return translations[:expected_count]
        except json.JSONDecodeError:
            pass

        # Fallback: –ø–∞—Ä—Å–∏–º –ø–æ—Å—Ç—Ä–æ—á–Ω–æ (–µ—Å–ª–∏ LLM –≤–µ—Ä–Ω—É–ª –Ω–µ JSON)
        lines = []
        for line in cleaned.splitlines():
            line = line.strip()
            # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é "1. " –∏–ª–∏ "1) "
            line = re.sub(r'^\d+[\.\)]\s*', '', line)
            # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏
            line = line.strip('"\'')
            if line:
                lines.append(line)

        while len(lines) < expected_count:
            lines.append("")
        return lines[:expected_count]

    def _extract_translation(self, response: str, original: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ–¥–∏–Ω–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å."""
        cleaned = response.strip()
        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –µ—Å–ª–∏ –æ–±—ë—Ä–Ω—É—Ç
        if (cleaned.startswith('"') and cleaned.endswith('"')) or \
           (cleaned.startswith("'") and cleaned.endswith("'")):
            cleaned = cleaned[1:-1]
        return cleaned or original

    @staticmethod
    def _create_batches(items: List, batch_size: int) -> List[List]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞ –±–∞—Ç—á–∏."""
        return [items[i:i + batch_size] for i in range(0, len(items), batch_size)]
