#!/usr/bin/env python3
"""
Validator - –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Ç—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤.

–î–≤—É—Ö—Ñ–∞–∑–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è:
1. Python (–∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è) - —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç:
   - –ù–∞–ª–∏—á–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
   - –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
   - –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏
   - –ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ —Ñ–∞–π–ª–∞–º
   - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –Ω–µ–ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫

2. LLM (–≤—ã–±–æ—Ä–æ—á–Ω–∞—è) - –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç—á—ë—Ç –∏:
   - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
   - –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º
   - –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ç–æ—á–µ—á–Ω—ã–µ –ø—Ä–∞–≤–∫–∏ –∫ –∫–∞—Ç–∞–ª–æ–≥—É
"""

import json
import re
import random
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field, asdict

from .catalog import TranslationCatalog, TranslationEntry
from .scanner import ProjectScanner, detect_language, has_translatable_content


# ============================================================================
# –§–ê–ó–ê 1: –ê–õ–ì–û–†–ò–¢–ú–ò–ß–ï–°–ö–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø (Python)
# ============================================================================

@dataclass
class ValidationIssue:
    """–ü—Ä–æ–±–ª–µ–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    severity: str           # critical | warning | info
    category: str           # missing | placeholder | consistency | quality | orphan
    original: str
    translated: str = ""
    message: str = ""
    file: str = ""
    line: int = 0
    suggestion: str = ""


@dataclass
class ValidationReport:
    """–û—Ç—á—ë—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    timestamp: str = ""
    source_locale: str = ""
    target_locale: str = ""

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_strings: int = 0
    translated_count: int = 0
    missing_count: int = 0
    coverage_percent: float = 0.0

    # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è
    by_category: Dict = field(default_factory=dict)
    by_file: Dict = field(default_factory=dict)
    by_severity: Dict = field(default_factory=dict)

    # –ü—Ä–æ–±–ª–µ–º—ã
    issues: List[dict] = field(default_factory=list)

    # –î–ª—è LLM
    sample_for_review: List[dict] = field(default_factory=list)
    llm_corrections: List[dict] = field(default_factory=list)

    def to_dict(self) -> Dict:
        return asdict(self)

    def to_json(self, compact: bool = False) -> str:
        if compact:
            return json.dumps(self.to_dict(), ensure_ascii=False, separators=(',', ':'))
        return json.dumps(self.to_dict(), ensure_ascii=False, indent=2)

    @property
    def has_critical_issues(self) -> bool:
        return self.by_severity.get("critical", 0) > 0

    @property
    def is_complete(self) -> bool:
        return self.missing_count == 0 and not self.has_critical_issues


class TranslationValidator:
    """
    –í–∞–ª–∏–¥–∞—Ç–æ—Ä –ø–µ—Ä–µ–≤–æ–¥–æ–≤.

    –§–∞–∑–∞ 1 (Python):
    - –ü–æ–ª–Ω–æ—Ç–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏
    - –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
    - –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–æ–≤
    - –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤
    - –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞

    –§–∞–∑–∞ 2 (LLM):
    - –í—ã–±–æ—Ä–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    - –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
    - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–æ—á–µ—á–Ω—ã—Ö –ø—Ä–∞–≤–æ–∫
    """

    def __init__(self, catalog: TranslationCatalog,
                 project_root: Optional[Path] = None):
        self.catalog = catalog
        self.project_root = project_root or Path.cwd()

        # –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≥–ª–æ—Å—Å–∞—Ä–∏–π (–æ–∂–∏–¥–∞–µ–º—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤)
        self.glossary = {
            "en_to_ru": {
                "security": "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç",
                "performance": "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç",
                "coverage": "–ø–æ–∫—Ä—ã—Ç",
                "vulnerability": "—É—è–∑–≤–∏–º–æ—Å—Ç",
                "issue": "–ø—Ä–æ–±–ª–µ–º",
                "error": "–æ—à–∏–±–∫",
                "warning": "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω",
                "recommendation": "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏",
                "analysis": "–∞–Ω–∞–ª–∏–∑",
                "report": "–æ—Ç—á—ë—Ç",
                "test": "—Ç–µ—Å—Ç",
                "quality": "–∫–∞—á–µ—Å—Ç–≤",
            },
            "ru_to_en": {
                "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å": "security",
                "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å": "performance",
                "–ø–æ–∫—Ä—ã—Ç–∏–µ": "coverage",
                "—É—è–∑–≤–∏–º–æ—Å—Ç—å": "vulnerability",
                "–ø—Ä–æ–±–ª–µ–º–∞": "issue",
                "–æ—à–∏–±–∫–∞": "error",
                "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ": "warning",
                "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è": "recommendation",
                "–∞–Ω–∞–ª–∏–∑": "analysis",
                "–æ—Ç—á—ë—Ç": "report",
                "—Ç–µ—Å—Ç": "test",
                "–∫–∞—á–µ—Å—Ç–≤–æ": "quality",
            }
        }

    # =========================================================================
    # –§–ê–ó–ê 1: –ê–õ–ì–û–†–ò–¢–ú–ò–ß–ï–°–ö–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø
    # =========================================================================

    def validate(self, source_locale: str, target_locale: str,
                 sample_size: int = 20) -> ValidationReport:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–µ—Ä–µ–≤–æ–¥–æ–≤.

        Args:
            source_locale: –ò—Å—Ö–æ–¥–Ω—ã–π —è–∑—ã–∫
            target_locale: –¶–µ–ª–µ–≤–æ–π —è–∑—ã–∫
            sample_size: –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è LLM-–ø—Ä–æ–≤–µ—Ä–∫–∏

        Returns:
            ValidationReport —Å –ø–æ–ª–Ω—ã–º –æ—Ç—á—ë—Ç–æ–º
        """
        report = ValidationReport(
            timestamp=datetime.now().isoformat(),
            source_locale=source_locale,
            target_locale=target_locale,
        )

        entries = self.catalog.load_full(target_locale)
        if not entries:
            report.total_strings = 0
            report.issues.append(asdict(ValidationIssue(
                severity="critical",
                category="missing",
                original="",
                message=f"–ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è –ª–æ–∫–∞–ª–∏ '{target_locale}' –ø—É—Å—Ç –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
            )))
            return report

        report.total_strings = len(entries)

        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø–µ—Ä–µ–≤–æ–¥–æ–≤
        self._check_completeness(entries, report)

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
        self._check_placeholders(entries, report)

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤
        self._check_terminology(entries, source_locale, target_locale, report)

        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤
        self._check_mixed_languages(entries, target_locale, report)

        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤ (–∞–Ω–æ–º–∞–ª–∏–∏)
        self._check_length_anomalies(entries, report)

        # 6. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ —Ñ–∞–π–ª–∞–º
        self._compute_category_stats(entries, report)
        self._compute_file_stats(entries, report)

        # 7. –ü–æ–¥—Å—á—ë—Ç severity
        severity_counts = {}
        for issue in report.issues:
            sev = issue.get("severity", "info")
            severity_counts[sev] = severity_counts.get(sev, 0) + 1
        report.by_severity = severity_counts

        # 8. –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è LLM
        report.sample_for_review = self._select_sample_for_llm(
            entries, report, sample_size
        )

        return report

    def _check_completeness(self, entries: Dict[str, TranslationEntry],
                            report: ValidationReport):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏."""
        translated = 0
        missing = 0

        for key, entry in entries.items():
            if entry.status == "orphaned":
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–¥–∞–ª—ë–Ω–Ω—ã–µ

            if entry.translated and entry.translated.strip():
                translated += 1
            else:
                missing += 1
                report.issues.append(asdict(ValidationIssue(
                    severity="critical",
                    category="missing",
                    original=key[:100],
                    message="–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–µ—Ä–µ–≤–æ–¥",
                    file=entry.source_file,
                    line=entry.source_line,
                )))

        report.translated_count = translated
        report.missing_count = missing
        report.coverage_percent = round(
            translated / max(len(entries), 1) * 100, 1
        )

    def _check_placeholders(self, entries: Dict[str, TranslationEntry],
                            report: ValidationReport):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–Ω–æ—Å—Ç—å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤ –≤ –ø–µ—Ä–µ–≤–æ–¥–∞—Ö."""
        placeholder_pattern = re.compile(r'\{[^}]*\}|%[sd]|%\([^)]+\)[sd]')

        for key, entry in entries.items():
            if not entry.translated:
                continue

            original_ph = set(placeholder_pattern.findall(key))
            translated_ph = set(placeholder_pattern.findall(entry.translated))

            if original_ph and original_ph != translated_ph:
                missing_ph = original_ph - translated_ph
                extra_ph = translated_ph - original_ph

                msg_parts = []
                if missing_ph:
                    msg_parts.append(f"–ø–æ—Ç–µ—Ä—è–Ω—ã: {', '.join(missing_ph)}")
                if extra_ph:
                    msg_parts.append(f"–¥–æ–±–∞–≤–ª–µ–Ω—ã: {', '.join(extra_ph)}")

                report.issues.append(asdict(ValidationIssue(
                    severity="critical",
                    category="placeholder",
                    original=key[:80],
                    translated=entry.translated[:80],
                    message=f"–ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤: {'; '.join(msg_parts)}",
                    file=entry.source_file,
                    suggestion=f"–ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ: {', '.join(original_ph)}"
                )))

    def _check_terminology(self, entries: Dict[str, TranslationEntry],
                           source_locale: str, target_locale: str,
                           report: ValidationReport):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤."""
        direction = f"{source_locale}_to_{target_locale}"
        glossary = self.glossary.get(direction, {})

        if not glossary:
            return

        for key, entry in entries.items():
            if not entry.translated:
                continue

            for source_term, expected_root in glossary.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Ä–º–∏–Ω
                if source_term.lower() not in key.lower():
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –∫–æ—Ä–Ω—è –≤ –ø–µ—Ä–µ–≤–æ–¥–µ
                if expected_root.lower() not in entry.translated.lower():
                    report.issues.append(asdict(ValidationIssue(
                        severity="warning",
                        category="consistency",
                        original=key[:80],
                        translated=entry.translated[:80],
                        message=f"–¢–µ—Ä–º–∏–Ω '{source_term}' –æ–±—ã—á–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è —Å –∫–æ—Ä–Ω–µ–º '{expected_root}'",
                        suggestion=f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–µ—Ä–µ–≤–æ–¥ —Ç–µ—Ä–º–∏–Ω–∞ '{source_term}'"
                    )))

    def _check_mixed_languages(self, entries: Dict[str, TranslationEntry],
                                target_locale: str,
                                report: ValidationReport):
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –Ω–µ–ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤–Ω—É—Ç—Ä–∏ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫."""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π –∞–ª—Ñ–∞–≤–∏—Ç –æ–∂–∏–¥–∞–µ—Ç—Å—è
        if target_locale == "ru":
            # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ - –∏—â–µ–º –¥–ª–∏–Ω–Ω—ã–µ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ —Ñ—Ä–∞–∑—ã (–Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã)
            unexpected_pattern = re.compile(
                r'(?<![A-Z_])\b[a-z]{3,}\s+[a-z]{3,}(?:\s+[a-z]{3,})*\b'
            )
        elif target_locale == "en":
            # –î–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ - –∏—â–µ–º –∫–∏—Ä–∏–ª–ª–∏—Ü—É
            unexpected_pattern = re.compile(r'[–∞-—è–ê-–Ø—ë–Å]{3,}')
        else:
            return

        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å
        tech_terms = {
            "sql", "api", "json", "yaml", "xml", "html", "css", "http", "https",
            "pytest", "bandit", "pylint", "radon", "flake8", "semgrep", "coverage",
            "cve", "xss", "csrf", "dry", "solid", "cprofile", "memory_profiler",
            "pydeps", "mccabe", "ruff", "mypy", "safety", "pip", "npm",
            "git", "docker", "redis", "nginx", "postgresql", "mongodb",
        }

        for key, entry in entries.items():
            if not entry.translated:
                continue

            matches = unexpected_pattern.findall(entry.translated)
            non_tech_matches = [
                m for m in matches
                if m.lower().strip() not in tech_terms and len(m) > 5
            ]

            if non_tech_matches:
                report.issues.append(asdict(ValidationIssue(
                    severity="warning",
                    category="quality",
                    original=key[:80],
                    translated=entry.translated[:80],
                    message=f"–í–æ–∑–º–æ–∂–Ω–æ –Ω–µ–ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã: {', '.join(non_tech_matches[:3])}",
                    suggestion="–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–ª–Ω–æ—Ç—É –ø–µ—Ä–µ–≤–æ–¥–∞"
                )))

    def _check_length_anomalies(self, entries: Dict[str, TranslationEntry],
                                 report: ValidationReport):
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –¥–ª–∏–Ω—ã –ø–µ—Ä–µ–≤–æ–¥–æ–≤."""
        for key, entry in entries.items():
            if not entry.translated:
                continue

            orig_len = len(key)
            trans_len = len(entry.translated)

            # –ü–µ—Ä–µ–≤–æ–¥ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–æ—á–µ (–≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ–ø–æ–ª–Ω—ã–π)
            if orig_len > 20 and trans_len < orig_len * 0.3:
                report.issues.append(asdict(ValidationIssue(
                    severity="warning",
                    category="quality",
                    original=key[:60],
                    translated=entry.translated[:60],
                    message=f"–ü–µ—Ä–µ–≤–æ–¥ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–π ({trans_len} vs {orig_len} —Å–∏–º–≤–æ–ª–æ–≤)",
                    suggestion="–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–ª–Ω–æ—Ç—É –ø–µ—Ä–µ–≤–æ–¥–∞"
                )))

            # –ü–µ—Ä–µ–≤–æ–¥ = –æ—Ä–∏–≥–∏–Ω–∞–ª (–Ω–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ?)
            if key == entry.translated and detect_language(key) != "unknown":
                lang = detect_language(key)
                if lang not in ("unknown", "mixed"):
                    report.issues.append(asdict(ValidationIssue(
                        severity="info",
                        category="quality",
                        original=key[:60],
                        message="–ü–µ—Ä–µ–≤–æ–¥ –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª—É",
                        suggestion="–í–æ–∑–º–æ–∂–Ω–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –±—ã–ª–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞"
                    )))

    def _compute_category_stats(self, entries: Dict[str, TranslationEntry],
                                 report: ValidationReport):
        """–°—á–∏—Ç–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º."""
        cats: Dict[str, Dict] = {}

        for entry in entries.values():
            cat = entry.category or "unknown"
            if cat not in cats:
                cats[cat] = {"total": 0, "translated": 0, "missing": 0}
            cats[cat]["total"] += 1
            if entry.translated:
                cats[cat]["translated"] += 1
            else:
                cats[cat]["missing"] += 1

        for cat, data in cats.items():
            data["coverage"] = round(
                data["translated"] / max(data["total"], 1) * 100, 1
            )

        report.by_category = cats

    def _compute_file_stats(self, entries: Dict[str, TranslationEntry],
                             report: ValidationReport):
        """–°—á–∏—Ç–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ñ–∞–π–ª–∞–º."""
        files: Dict[str, Dict] = {}

        for entry in entries.values():
            f = entry.source_file or "unknown"
            if f not in files:
                files[f] = {"total": 0, "translated": 0, "missing": 0}
            files[f]["total"] += 1
            if entry.translated:
                files[f]["translated"] += 1
            else:
                files[f]["missing"] += 1

        for f, data in files.items():
            data["coverage"] = round(
                data["translated"] / max(data["total"], 1) * 100, 1
            )

        report.by_file = files

    def _select_sample_for_llm(self, entries: Dict[str, TranslationEntry],
                                report: ValidationReport,
                                sample_size: int) -> List[dict]:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –≤—ã–±–æ—Ä–∫—É –¥–ª—è LLM-–ø—Ä–æ–≤–µ—Ä–∫–∏.

        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å—Ç—Ä–æ–∫–∞–º —Å warning/info issues + —Å–ª—É—á–∞–π–Ω—ã–µ.
        """
        sample = []

        # 1. –°—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏ (–Ω–µ critical - —Ç–µ —É–∂–µ –æ—á–µ–≤–∏–¥–Ω—ã)
        issue_keys = set()
        for issue in report.issues:
            if issue.get("severity") in ("warning", "info"):
                key = issue.get("original", "")
                if key and key in entries and entries[key].translated:
                    issue_keys.add(key)

        for key in list(issue_keys)[:sample_size // 2]:
            entry = entries[key]
            sample.append({
                "original": key,
                "translated": entry.translated,
                "category": entry.category,
                "context": entry.context,
                "file": entry.source_file,
            })

        # 2. –°–ª—É—á–∞–π–Ω—ã–µ –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        remaining = sample_size - len(sample)
        translated_keys = [
            k for k, v in entries.items()
            if v.translated and k not in issue_keys
        ]
        random_keys = random.sample(
            translated_keys, min(remaining, len(translated_keys))
        )

        for key in random_keys:
            entry = entries[key]
            sample.append({
                "original": key,
                "translated": entry.translated,
                "category": entry.category,
                "context": entry.context,
                "file": entry.source_file,
            })

        return sample

    # =========================================================================
    # –§–ê–ó–ê 2: LLM –ü–†–û–í–ï–†–ö–ê –ò –ü–†–ê–í–ö–ò
    # =========================================================================

    def llm_review(self, report: ValidationReport,
                   source_locale: str, target_locale: str,
                   llm_backend: str = "gemini",
                   llm_model: str = "pro",
                   gemini_bridge_path: str = "") -> ValidationReport:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ—Ç—á—ë—Ç + –≤—ã–±–æ—Ä–∫—É –≤ LLM –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞.

        LLM –ø–æ–ª—É—á–∞–µ—Ç:
        - –°–≤–æ–¥–∫—É –æ—Ç—á—ë—Ç–∞ (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –ø—Ä–æ–±–ª–µ–º—ã)
        - –í—ã–±–æ—Ä–∫—É –ø–µ—Ä–µ–≤–æ–¥–æ–≤
        - –ü—Ä–æ—Å—å–±—É –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è

        Returns:
            –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π –æ—Ç—á—ë—Ç —Å llm_corrections
        """
        if not report.sample_for_review:
            print("  –ù–µ—Ç —Å—Ç—Ä–æ–∫ –¥–ª—è LLM-–ø—Ä–æ–≤–µ—Ä–∫–∏")
            return report

        prompt = self._build_review_prompt(report, source_locale, target_locale)

        try:
            response = self._call_review_llm(
                prompt, llm_backend, llm_model, gemini_bridge_path
            )
            corrections = self._parse_review_response(response)
            report.llm_corrections = corrections

            print(f"  LLM –ø—Ä–µ–¥–ª–æ–∂–∏–ª {len(corrections)} –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π")

        except Exception as e:
            print(f"  –û—à–∏–±–∫–∞ LLM-–ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")
            report.llm_corrections = []

        return report

    def apply_corrections(self, report: ValidationReport,
                          target_locale: str,
                          auto_apply: bool = False) -> int:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç LLM-–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫ –∫–∞—Ç–∞–ª–æ–≥—É.

        Args:
            report: –û—Ç—á—ë—Ç —Å llm_corrections
            target_locale: –¶–µ–ª–µ–≤–∞—è –ª–æ–∫–∞–ª—å
            auto_apply: –ü—Ä–∏–º–µ–Ω–∏—Ç—å –±–µ–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        """
        if not report.llm_corrections:
            return 0

        entries = self.catalog.load_full(target_locale)
        applied = 0

        for correction in report.llm_corrections:
            original = correction.get("original", "")
            new_translation = correction.get("corrected", "")
            reason = correction.get("reason", "")

            if not original or not new_translation:
                continue

            if original not in entries:
                continue

            old_translation = entries[original].translated

            if auto_apply:
                entries[original].translated = new_translation
                entries[original].status = "reviewed"
                entries[original].translator = "llm_review"
                entries[original].updated_at = datetime.now().isoformat()
                applied += 1
                print(f"    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: '{old_translation[:40]}' -> '{new_translation[:40]}' ({reason})")
            else:
                print(f"\n  –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:")
                print(f"    –û—Ä–∏–≥–∏–Ω–∞–ª:  {original[:60]}")
                print(f"    –ë—ã–ª–æ:      {old_translation[:60]}")
                print(f"    –°—Ç–∞–ª–æ:     {new_translation[:60]}")
                print(f"    –ü—Ä–∏—á–∏–Ω–∞:   {reason}")

        if applied > 0:
            self.catalog.save(target_locale, entries)

        return applied

    def _build_review_prompt(self, report: ValidationReport,
                             source_locale: str, target_locale: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LLM-–ø—Ä–æ–≤–µ—Ä–∫–∏."""
        locale_names = {
            "ru": "—Ä—É—Å—Å–∫–∏–π", "en": "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π", "de": "–Ω–µ–º–µ—Ü–∫–∏–π",
            "fr": "—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π", "es": "–∏—Å–ø–∞–Ω—Å–∫–∏–π",
        }
        source_name = locale_names.get(source_locale, source_locale)
        target_name = locale_names.get(target_locale, target_locale)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É –æ—Ç—á—ë—Ç–∞
        summary = (
            f"–ü—Ä–æ–µ–∫—Ç: —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ (IT-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç)\n"
            f"–ü–µ—Ä–µ–≤–æ–¥: {source_name} -> {target_name}\n"
            f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {report.total_strings}\n"
            f"–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: {report.translated_count} ({report.coverage_percent}%)\n"
            f"–ü—Ä–æ–±–ª–µ–º: {len(report.issues)} "
            f"(critical: {report.by_severity.get('critical', 0)}, "
            f"warning: {report.by_severity.get('warning', 0)})\n"
        )

        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        issues_text = ""
        warning_issues = [i for i in report.issues if i.get("severity") == "warning"]
        if warning_issues:
            issues_lines = []
            for issue in warning_issues[:10]:
                issues_lines.append(
                    f"  - [{issue.get('category')}] '{issue.get('original', '')[:50]}' -> "
                    f"'{issue.get('translated', '')[:50]}': {issue.get('message', '')}"
                )
            issues_text = "–û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´:\n" + "\n".join(issues_lines)

        # –í—ã–±–æ—Ä–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        sample_lines = []
        for item in report.sample_for_review:
            sample_lines.append(
                f"  –û—Ä–∏–≥–∏–Ω–∞–ª ({source_locale}): {item['original'][:80]}\n"
                f"  –ü–µ—Ä–µ–≤–æ–¥ ({target_locale}): {item['translated'][:80]}\n"
                f"  –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {item.get('category', 'unknown')}\n"
            )
        sample_text = "\n".join(sample_lines)

        prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ IT-–ø—Ä–æ–¥—É–∫—Ç–æ–≤.

–û–¢–ß–Å–¢ –í–ê–õ–ò–î–ê–¶–ò–ò:
{summary}

{issues_text}

–í–´–ë–û–†–ö–ê –ü–ï–†–ï–í–û–î–û–í –î–õ–Ø –ü–†–û–í–ï–†–ö–ò:
{sample_text}

–ó–ê–î–ê–ß–ê:
1. –ü—Ä–æ–≤–µ—Ä—å –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ
2. –ù–∞–π–¥–∏ –æ—à–∏–±–∫–∏: –Ω–µ—Ç–æ—á–Ω—ã–µ –ø–µ—Ä–µ–≤–æ–¥—ã, —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã, –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
3. –ü—Ä–µ–¥–ª–æ–∂–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¢–û–õ–¨–ö–û –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê - JSON –º–∞—Å—Å–∏–≤ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:
[
  {{
    "original": "–∏—Å—Ö–æ–¥–Ω–∞—è —Å—Ç—Ä–æ–∫–∞",
    "corrected": "–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥",
    "reason": "–ø—Ä–∏—á–∏–Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–∫—Ä–∞—Ç–∫–æ)"
  }}
]

–ï—Å–ª–∏ –≤—Å–µ –ø–µ—Ä–µ–≤–æ–¥—ã —Ö–æ—Ä–æ—à–∏–µ, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤: []
–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."""

        return prompt

    def _call_review_llm(self, prompt: str, backend: str,
                         model: str, bridge_path: str) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç LLM –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏."""
        if backend == "gemini":
            if bridge_path and Path(bridge_path).exists():
                result = subprocess.run(
                    ["bash", bridge_path, model, prompt],
                    capture_output=True, text=True, timeout=120
                )
                if result.returncode == 0:
                    return result.stdout.strip()
                raise Exception(f"Gemini bridge error: {result.stderr}")
            else:
                # –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ gemini CLI
                try:
                    result = subprocess.run(
                        ["gemini", "-m", f"gemini-2.5-{model}", "-p", prompt],
                        capture_output=True, text=True, timeout=120
                    )
                    if result.returncode == 0:
                        return result.stdout.strip()
                    raise Exception(f"Gemini CLI error: {result.stderr}")
                except FileNotFoundError:
                    raise Exception("Gemini CLI –Ω–µ –Ω–∞–π–¥–µ–Ω")
        else:
            raise Exception(f"Backend '{backend}' –¥–ª—è review –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∞–Ω")

    def _parse_review_response(self, response: str) -> List[dict]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç LLM —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏."""
        cleaned = response.strip()
        if cleaned.startswith("```"):
            cleaned = re.sub(r'```(?:json)?\n?', '', cleaned).strip()

        try:
            corrections = json.loads(cleaned)
            if isinstance(corrections, list):
                # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç
                valid = []
                for c in corrections:
                    if isinstance(c, dict) and "original" in c and "corrected" in c:
                        valid.append({
                            "original": c["original"],
                            "corrected": c["corrected"],
                            "reason": c.get("reason", ""),
                        })
                return valid
        except json.JSONDecodeError:
            pass

        return []

    # =========================================================================
    # –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–Å–¢–ê
    # =========================================================================

    def print_report(self, report: ValidationReport):
        """–í—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç –≤ –∫–æ–Ω—Å–æ–ª—å."""
        print(f"\n{'='*70}")
        print(f"  –û–¢–ß–Å–¢ –í–ê–õ–ò–î–ê–¶–ò–ò –ü–ï–†–ï–í–û–î–û–í")
        print(f"  {report.source_locale} -> {report.target_locale}")
        print(f"  {report.timestamp}")
        print(f"{'='*70}\n")

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        status = "PASS" if report.is_complete else "FAIL"
        emoji = "‚úÖ" if report.is_complete else "‚ùå"
        print(f"  {emoji} –°—Ç–∞—Ç—É—Å: {status}")
        print(f"  –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫:     {report.total_strings}")
        print(f"  –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ:      {report.translated_count}")
        print(f"  –ù–µ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ:   {report.missing_count}")
        print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ:        {report.coverage_percent}%")

        # –ü—Ä–æ–±–ª–µ–º—ã –ø–æ severity
        if report.by_severity:
            print(f"\n  –ü—Ä–æ–±–ª–µ–º—ã:")
            for sev, count in sorted(report.by_severity.items()):
                icon = {"critical": "üî¥", "warning": "üü°", "info": "üîµ"}.get(sev, "‚ö™")
                print(f"    {icon} {sev}: {count}")

        # –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if report.by_category:
            print(f"\n  –ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
            for cat, data in sorted(report.by_category.items()):
                bar = self._progress_bar(data.get("coverage", 0))
                print(f"    {cat:<20} {bar} {data.get('coverage', 0)}% "
                      f"({data.get('translated', 0)}/{data.get('total', 0)})")

        # –ü–æ —Ñ–∞–π–ª–∞–º
        if report.by_file:
            print(f"\n  –ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ —Ñ–∞–π–ª–∞–º:")
            for f, data in sorted(report.by_file.items(),
                                   key=lambda x: x[1].get("coverage", 0)):
                bar = self._progress_bar(data.get("coverage", 0))
                print(f"    {f:<35} {bar} {data.get('coverage', 0)}%")

        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã (–ø–µ—Ä–≤—ã–µ 10)
        critical = [i for i in report.issues if i.get("severity") == "critical"]
        if critical:
            print(f"\n  üî¥ –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã ({len(critical)}):")
            for issue in critical[:10]:
                print(f"    [{issue.get('category')}] {issue.get('message')}")
                print(f"      –°—Ç—Ä–æ–∫–∞: '{issue.get('original', '')[:60]}'")
                if issue.get("file"):
                    print(f"      –§–∞–π–ª: {issue.get('file')}:{issue.get('line', '')}")

        # LLM-–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        if report.llm_corrections:
            print(f"\n  ü§ñ LLM –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è ({len(report.llm_corrections)}):")
            for c in report.llm_corrections:
                print(f"    –û—Ä–∏–≥–∏–Ω–∞–ª:   {c.get('original', '')[:50]}")
                print(f"    –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: {c.get('corrected', '')[:50]}")
                print(f"    –ü—Ä–∏—á–∏–Ω–∞:    {c.get('reason', '')}")
                print()

        print(f"{'='*70}\n")

    def save_report(self, report: ValidationReport, output_path: Path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á—ë—Ç –≤ JSON-—Ñ–∞–π–ª."""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report.to_json())
        print(f"  –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

    @staticmethod
    def _progress_bar(percent: float, width: int = 20) -> str:
        """–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä."""
        filled = int(width * percent / 100)
        bar = "‚ñà" * filled + "‚ñë" * (width - filled)
        return f"[{bar}]"
