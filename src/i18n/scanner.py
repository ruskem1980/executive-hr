#!/usr/bin/env python3
"""
Scanner - –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∏–º—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–¥–∞ –ø—Ä–æ–µ–∫—Ç–∞.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Python AST –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ .py —Ñ–∞–π–ª–æ–≤ –∏ regex –¥–ª—è .yaml/.json.
–ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM - —á–∏—Å—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞.

–°—Ç—Ä–∞—Ç–µ–≥–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:
1. print() / logging.xxx() –≤—ã–∑–æ–≤—ã - –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
2. f-—Å—Ç—Ä–æ–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
3. Enum –∑–Ω–∞—á–µ–Ω–∏—è —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
4. –°—Ç—Ä–æ–∫–æ–≤—ã–µ –ª–∏—Ç–µ—Ä–∞–ª—ã –≤ argparse (help, description)
5. –°—Ç—Ä–æ–∫–∏-—à–∞–±–ª–æ–Ω—ã –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤ (summary, recommendations)
6. YAML-–∫–æ–Ω—Ñ–∏–≥–∏: –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
"""

import ast
import re
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum


class StringCategory(Enum):
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫."""
    UI_MESSAGE = "ui_message"           # print(), user-facing
    LOG_MESSAGE = "log_message"         # logging.xxx()
    ERROR_MESSAGE = "error_message"     # raise, error strings
    ENUM_VALUE = "enum_value"           # Enum member values
    CLI_HELP = "cli_help"              # argparse help/description
    REPORT_TEMPLATE = "report_template" # summary, recommendations
    CONFIG_LABEL = "config_label"       # YAML/JSON labels
    DOCSTRING = "docstring"             # Module/class/function docstrings
    COMMENT = "comment"                 # Inline comments (Russian)
    KEYWORD = "keyword"                 # Classification keywords
    STATUS_TEXT = "status_text"         # Status indicators with text


@dataclass
class ExtractedString:
    """–ò–∑–≤–ª–µ—á—ë–Ω–Ω–∞—è –ø–µ—Ä–µ–≤–æ–¥–∏–º–∞—è —Å—Ç—Ä–æ–∫–∞."""
    text: str
    category: str
    file: str
    line: int
    context: str = ""          # –û–∫—Ä—É–∂–∞—é—â–∏–π –∫–æ–¥ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    string_id: str = ""        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ö–µ—à-ID
    has_placeholders: bool = False  # –°–æ–¥–µ—Ä–∂–∏—Ç {}, %s –∏ —Ç.–¥.
    language_detected: str = ""     # 'ru', 'en', 'mixed', 'unknown'

    def __post_init__(self):
        if not self.string_id:
            raw = f"{self.file}:{self.line}:{self.text}"
            self.string_id = hashlib.md5(raw.encode()).hexdigest()[:12]
        if not self.language_detected:
            self.language_detected = detect_language(self.text)


def detect_language(text: str) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Å—Ç—Ä–æ–∫–∏ (ru/en/mixed/unknown).
    –ë–µ–∑ LLM - –ø–æ –Ω–∞–ª–∏—á–∏—é –∫–∏—Ä–∏–ª–ª–∏—Ü—ã/–ª–∞—Ç–∏–Ω–∏—Ü—ã.
    """
    has_cyrillic = bool(re.search(r'[–∞-—è–ê-–Ø—ë–Å]', text))
    has_latin = bool(re.search(r'[a-zA-Z]', text))

    if has_cyrillic and has_latin:
        return "mixed"
    elif has_cyrillic:
        return "ru"
    elif has_latin:
        return "en"
    return "unknown"


def has_translatable_content(text: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç.
    –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ (–ø—É—Ç–∏, URL, regex, format-specifiers).
    """
    stripped = text.strip()

    # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
    if len(stripped) < 2:
        return False

    # –ß–∏—Å—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã - –ù–ï –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å
    skip_patterns = [
        r'^[\s\-=\*_\#\|]+$',           # –¢–æ–ª—å–∫–æ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏)
        r'^[/\\][\w/\\.]+$',             # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
        r'^https?://',                    # URL
        r'^\w+\.\w+\.\w+',              # Dotted identifiers (a.b.c)
        r'^%[\(\w\)]*[sdif]',            # printf-style format only
        r'^\{[\w\.:!]*\}$',             # –¢–æ–ª—å–∫–æ placeholder {name}
        r'^[\d\.\,\s\%\$]+$',           # –¢–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
        r'^[A-Z_]+$',                    # CONSTANT_NAMES
        r'^[\w\-]+\.(?:py|js|yaml|json|md|sh|log)$',  # –ò–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
        r'^(?:src|tests|config|docs)/',  # –ü—É—Ç–∏ –≤ –ø—Ä–æ–µ–∫—Ç–µ
        r'^--[\w\-]+=?',                 # CLI —Ñ–ª–∞–≥–∏
        r'^\w+://\w+',                   # –ü—Ä–æ—Ç–æ–∫–æ–ª—ã
        r'^[{}\[\]()]+$',               # –¢–æ–ª—å–∫–æ —Å–∫–æ–±–∫–∏
        r'^\.+$',                        # –¢–æ–ª—å–∫–æ —Ç–æ—á–∫–∏
        r'^utf-?\d+$',                   # –ö–æ–¥–∏—Ä–æ–≤–∫–∏
        r'^\d+\.\d+\.\d+',              # –í–µ—Ä—Å–∏–∏
        r'^[A-Z][a-z]+(?:[A-Z][a-z]+)+$',  # CamelCase –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
    ]

    for pattern in skip_patterns:
        if re.match(pattern, stripped):
            return False

    # –î–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –±—É–∫–≤—É (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞ –∏–ª–∏ –ª–∞—Ç–∏–Ω–∏—Ü–∞)
    if not re.search(r'[a-zA-Z–∞-—è–ê-–Ø—ë–Å]', stripped):
        return False

    # –î–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–º–∏–Ω–∏–º—É–º 2 –±—É–∫–≤—ã –ø–æ–¥—Ä—è–¥)
    if not re.search(r'[a-zA-Z–∞-—è–ê-–Ø—ë–Å]{2,}', stripped):
        return False

    return True


class PythonStringExtractor(ast.NodeVisitor):
    """AST-–≤–∏–∑–∏—Ç–æ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥–∏–º—ã—Ö —Å—Ç—Ä–æ–∫ –∏–∑ Python –∫–æ–¥–∞."""

    def __init__(self, file_path: str, source_lines: List[str]):
        self.file_path = file_path
        self.source_lines = source_lines
        self.strings: List[ExtractedString] = []
        self._current_class = ""
        self._current_func = ""
        self._in_enum = False

    def visit_ClassDef(self, node: ast.ClassDef):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∫–ª–∞—Å—Å—ã, –æ—Å–æ–±–µ–Ω–Ω–æ Enum."""
        old_class = self._current_class
        self._current_class = node.name

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ Enum
        old_in_enum = self._in_enum
        for base in node.bases:
            if isinstance(base, ast.Name) and base.id == "Enum":
                self._in_enum = True
            elif isinstance(base, ast.Attribute) and base.attr == "Enum":
                self._in_enum = True

        self.generic_visit(node)
        self._current_class = old_class
        self._in_enum = old_in_enum

    def visit_FunctionDef(self, node: ast.FunctionDef):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç docstrings."""
        old_func = self._current_func
        self._current_func = node.name

        # Docstring
        if (node.body and isinstance(node.body[0], ast.Expr) and
                isinstance(node.body[0].value, (ast.Constant, ast.Constant))):
            docstring = self._get_str_value(node.body[0].value)
            if docstring and has_translatable_content(docstring):
                self._add_string(
                    docstring, StringCategory.DOCSTRING,
                    node.body[0].lineno,
                    context=f"def {node.name}()"
                )

        self.generic_visit(node)
        self._current_func = old_func

    visit_AsyncFunctionDef = visit_FunctionDef

    def visit_Assign(self, node: ast.Assign):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–π (Enum members, constants)."""
        if self._in_enum:
            for target in node.targets:
                if isinstance(target, ast.Name) and isinstance(node.value, (ast.Constant, ast.Constant)):
                    val = self._get_str_value(node.value)
                    if val and has_translatable_content(val):
                        self._add_string(
                            val, StringCategory.ENUM_VALUE,
                            node.lineno,
                            context=f"{self._current_class}.{target.id}"
                        )
        self.generic_visit(node)

    def visit_Call(self, node: ast.Call):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π."""
        func_name = self._get_call_name(node)

        if func_name == "print":
            self._extract_call_strings(node, StringCategory.UI_MESSAGE)
        elif func_name in ("logger.info", "logger.warning", "logger.error",
                            "logger.debug", "logger.critical",
                            "logging.info", "logging.warning", "logging.error"):
            self._extract_call_strings(node, StringCategory.LOG_MESSAGE)
        elif func_name in ("parser.add_argument",):
            self._extract_argparse_strings(node)
        elif func_name in ("raise", "Exception", "ValueError",
                            "TypeError", "RuntimeError", "ImportError"):
            self._extract_call_strings(node, StringCategory.ERROR_MESSAGE)

        self.generic_visit(node)

    def visit_Raise(self, node: ast.Raise):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ raise."""
        if node.exc and isinstance(node.exc, ast.Call):
            self._extract_call_strings(node.exc, StringCategory.ERROR_MESSAGE)
        self.generic_visit(node)

    def visit_JoinedStr(self, node: ast.JoinedStr):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã f-—Å—Ç—Ä–æ–∫."""
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —á–∞—Å—Ç–∏ f-—Å—Ç—Ä–æ–∫–∏
        text_parts = []
        has_expr = False
        for value in node.values:
            if isinstance(value, (ast.Constant, ast.Constant)):
                text_parts.append(self._get_str_value(value))
            elif isinstance(value, ast.FormattedValue):
                text_parts.append("{...}")
                has_expr = True

        combined = "".join(text_parts)
        if has_translatable_content(combined):
            self._add_string(
                combined, StringCategory.UI_MESSAGE,
                node.lineno,
                context=f"f-string in {self._current_func or self._current_class}",
                has_placeholders=has_expr
            )
        self.generic_visit(node)

    def visit_Constant(self, node: ast.Constant):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –≤ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö."""
        if isinstance(node.value, str) and has_translatable_content(node.value):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ visit_Call, visit_Assign
            # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–¥–µ–ª—å–Ω–æ —Å—Ç–æ—è—â–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤ —Å–ø–∏—Å–∫–∞—Ö / —Å–ª–æ–≤–∞—Ä—è—Ö
            pass

    def _extract_call_strings(self, node: ast.Call, category: StringCategory):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–∏."""
        for arg in node.args:
            if isinstance(arg, (ast.Constant, ast.Constant)):
                val = self._get_str_value(arg)
                if val and has_translatable_content(val):
                    self._add_string(val, category, arg.lineno,
                                     context=self._get_call_name(node))
            elif isinstance(arg, ast.JoinedStr):
                # f-—Å—Ç—Ä–æ–∫–∞ - —Å–æ–±–∏—Ä–∞–µ–º —à–∞–±–ª–æ–Ω
                text_parts = []
                has_expr = False
                for value in arg.values:
                    if isinstance(value, (ast.Constant, ast.Constant)):
                        text_parts.append(self._get_str_value(value))
                    elif isinstance(value, ast.FormattedValue):
                        text_parts.append("{...}")
                        has_expr = True
                combined = "".join(text_parts)
                if has_translatable_content(combined):
                    self._add_string(
                        combined, category, arg.lineno,
                        context=self._get_call_name(node),
                        has_placeholders=has_expr
                    )

        # keyword arguments (e.g. help="...")
        for kw in node.keywords:
            if kw.arg in ("help", "description", "epilog", "metavar"):
                if isinstance(kw.value, (ast.Constant, ast.Constant)):
                    val = self._get_str_value(kw.value)
                    if val and has_translatable_content(val):
                        self._add_string(
                            val, StringCategory.CLI_HELP, kw.value.lineno,
                            context=f"argparse {kw.arg}="
                        )

    def _extract_argparse_strings(self, node: ast.Call):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ argparse –≤—ã–∑–æ–≤–æ–≤."""
        for kw in node.keywords:
            if kw.arg in ("help", "description", "epilog", "metavar"):
                if isinstance(kw.value, (ast.Constant, ast.Constant)):
                    val = self._get_str_value(kw.value)
                    if val and has_translatable_content(val):
                        self._add_string(
                            val, StringCategory.CLI_HELP, kw.value.lineno,
                            context=f"argparse.{kw.arg}"
                        )

    def _extract_list_strings(self, node: ast.List, category: StringCategory):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ —Å–ø–∏—Å–∫–æ–≤ (recommendations –∏ —Ç.–ø.)."""
        for elt in node.elts:
            if isinstance(elt, (ast.Constant, ast.Constant)):
                val = self._get_str_value(elt)
                if val and has_translatable_content(val):
                    self._add_string(val, category, elt.lineno)

    def _add_string(self, text: str, category: StringCategory, line: int,
                    context: str = "", has_placeholders: bool = False):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        self.strings.append(ExtractedString(
            text=text,
            category=category.value,
            file=self.file_path,
            line=line,
            context=context,
            has_placeholders=has_placeholders
        ))

    def _get_call_name(self, node: ast.Call) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–º—è –≤—ã–∑—ã–≤–∞–µ–º–æ–π —Ñ—É–Ω–∫—Ü–∏–∏."""
        if isinstance(node.func, ast.Name):
            return node.func.id
        elif isinstance(node.func, ast.Attribute):
            if isinstance(node.func.value, ast.Name):
                return f"{node.func.value.id}.{node.func.attr}"
            return node.func.attr
        return ""

    @staticmethod
    def _get_str_value(node) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ AST-—É–∑–ª–∞."""
        if isinstance(node, ast.Constant) and isinstance(node.value, str):
            return node.value
        return ""


class ProjectScanner:
    """
    –°–∫–∞–Ω–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞ - –æ–±—Ö–æ–¥–∏—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∏–º—ã–µ —Å—Ç—Ä–æ–∫–∏.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - .py —Ñ–∞–π–ª—ã (—á–µ—Ä–µ–∑ AST)
    - .yaml —Ñ–∞–π–ª—ã (—á–µ—Ä–µ–∑ regex)
    - –°—Ç—Ä–æ–∫–∏ –≤ —Å–ø–∏—Å–∫–∞—Ö/—Å–ª–æ–≤–∞—Ä—è—Ö (recommendations, keywords, etc.)
    """

    def __init__(self, project_root: Path, exclude_dirs: Optional[List[str]] = None):
        self.project_root = Path(project_root)
        self.exclude_dirs = exclude_dirs or [
            "__pycache__", ".git", ".venv", "venv", "node_modules",
            ".swarm", ".claude", "locales", "i18n"
        ]

    def scan(self, extensions: Optional[List[str]] = None) -> List[ExtractedString]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –ø—Ä–æ–µ–∫—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏.

        Args:
            extensions: –†–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.
                –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: ['.py', '.yaml', '.yml']

        Returns:
            –°–ø–∏—Å–æ–∫ ExtractedString —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏
        """
        extensions = extensions or [".py", ".yaml", ".yml"]
        all_strings: List[ExtractedString] = []

        for ext in extensions:
            files = self._find_files(ext)
            for file_path in files:
                if ext == ".py":
                    strings = self._scan_python_file(file_path)
                elif ext in (".yaml", ".yml"):
                    strings = self._scan_yaml_file(file_path)
                else:
                    continue
                all_strings.extend(strings)

        # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –∏–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ª–∏—Ç–µ—Ä–∞–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä—è—Ö/—Å–ø–∏—Å–∫–∞—Ö
        all_strings.extend(self._scan_inline_strings())

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ç–µ–∫—Å—Ç—É
        seen: Set[str] = set()
        unique: List[ExtractedString] = []
        for s in all_strings:
            if s.text not in seen:
                seen.add(s.text)
                unique.append(s)

        return unique

    def _find_files(self, extension: str) -> List[Path]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ñ–∞–π–ª—ã —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º."""
        files = []
        for path in self.project_root.rglob(f"*{extension}"):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–∫–ª—é—á—ë–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            parts = path.relative_to(self.project_root).parts
            if any(exc in parts for exc in self.exclude_dirs):
                continue
            files.append(path)
        return sorted(files)

    def _scan_python_file(self, file_path: Path) -> List[ExtractedString]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç Python —Ñ–∞–π–ª —á–µ—Ä–µ–∑ AST."""
        try:
            source = file_path.read_text(encoding="utf-8")
            source_lines = source.splitlines()
            tree = ast.parse(source, filename=str(file_path))

            rel_path = str(file_path.relative_to(self.project_root))
            extractor = PythonStringExtractor(rel_path, source_lines)
            extractor.visit(tree)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: —Å—Ç—Ä–æ–∫–∏ –∏–∑ recommendations –∏ –ø–æ–¥–æ–±–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤
            extra = self._extract_list_literals(tree, rel_path)
            extractor.strings.extend(extra)

            return extractor.strings

        except SyntaxError as e:
            print(f"  [SKIP] –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞: {file_path}: {e}")
            return []
        except Exception as e:
            print(f"  [SKIP] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {file_path}: {e}")
            return []

    def _extract_list_literals(self, tree: ast.Module, file_path: str) -> List[ExtractedString]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –∏–∑ –ª–∏—Ç–µ—Ä–∞–ª–æ–≤ —Å–ø–∏—Å–∫–æ–≤ (recommendations, keywords)."""
        strings = []

        for node in ast.walk(tree):
            # –ò—â–µ–º: "recommendations": [...]  –∏–ª–∏  recommendations.append("...")
            if isinstance(node, ast.List):
                for elt in node.elts:
                    if isinstance(elt, (ast.Constant, ast.Constant)):
                        val = elt.value if isinstance(elt, ast.Constant) else elt.s
                        if isinstance(val, str) and has_translatable_content(val):
                            strings.append(ExtractedString(
                                text=val,
                                category=StringCategory.REPORT_TEMPLATE.value,
                                file=file_path,
                                line=elt.lineno,
                                context="list literal"
                            ))

            # –ò—â–µ–º: dict values like {"msg": "Some message"}
            if isinstance(node, ast.Dict):
                for key, value in zip(node.keys, node.values):
                    if (isinstance(key, (ast.Constant, ast.Constant)) and
                            isinstance(value, (ast.Constant, ast.Constant))):
                        key_str = key.value if isinstance(key, ast.Constant) else key.s
                        val_str = value.value if isinstance(value, ast.Constant) else value.s
                        if (isinstance(key_str, str) and isinstance(val_str, str) and
                                key_str in ("msg", "message", "fix", "error",
                                           "summary", "description", "help") and
                                has_translatable_content(val_str)):
                            strings.append(ExtractedString(
                                text=val_str,
                                category=StringCategory.REPORT_TEMPLATE.value,
                                file=file_path,
                                line=value.lineno,
                                context=f"dict['{key_str}']"
                            ))

        return strings

    def _scan_yaml_file(self, file_path: Path) -> List[ExtractedString]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç YAML —Ñ–∞–π–ª —á–µ—Ä–µ–∑ regex (–±–µ–∑ pyyaml –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏)."""
        strings = []
        try:
            content = file_path.read_text(encoding="utf-8")
            rel_path = str(file_path.relative_to(self.project_root))

            for i, line in enumerate(content.splitlines(), 1):
                stripped = line.strip()

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏, —á–∏—Å—Ç—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏, –∫–ª—é—á–∏ –±–µ–∑ –∑–Ω–∞—á–µ–Ω–∏–π
                if not stripped or stripped.startswith("#"):
                    # –†—É—Å—Å–∫–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ - –ø–µ—Ä–µ–≤–æ–¥–∏–º—ã–µ
                    if stripped.startswith("#") and re.search(r'[–∞-—è–ê-–Ø—ë–Å]', stripped):
                        comment_text = stripped.lstrip("# ").strip()
                        if has_translatable_content(comment_text):
                            strings.append(ExtractedString(
                                text=comment_text,
                                category=StringCategory.COMMENT.value,
                                file=rel_path,
                                line=i,
                                context="YAML comment"
                            ))
                    continue

                # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ ":"
                match = re.match(r'^[\w\-]+:\s*["\']?(.+?)["\']?\s*$', stripped)
                if match:
                    value = match.group(1).strip("'\"")
                    if has_translatable_content(value) and not re.match(r'^[\d\.\-]+$', value):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                        if not re.match(r'^(true|false|null|none|\d+|[\w\-]+\.\w+)$', value, re.I):
                            strings.append(ExtractedString(
                                text=value,
                                category=StringCategory.CONFIG_LABEL.value,
                                file=rel_path,
                                line=i,
                                context="YAML value"
                            ))

        except Exception as e:
            print(f"  [SKIP] –û—à–∏–±–∫–∞ YAML: {file_path}: {e}")

        return strings

    def _scan_inline_strings(self) -> List[ExtractedString]:
        """
        –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ö–æ–¥: –∏—â–µ—Ç —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ AST –º–æ–≥ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å.
        Regex-–ø–æ–∏—Å–∫ –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º.
        """
        strings = []
        patterns = [
            # f"emoji text..."
            (r'(?:print|append)\s*\(\s*f?"([^"]*[–∞-—è–ê-–Ø—ë–Å][^"]*)"', StringCategory.UI_MESSAGE),
            # "emoji Status text"
            (r'(?:return|=)\s*f?"([^\n"]*(?:‚úÖ|‚ùå|‚ö†Ô∏è|üî¥|üü°|üü¢)[^\n"]*)"', StringCategory.STATUS_TEXT),
        ]

        for py_file in self._find_files(".py"):
            try:
                content = py_file.read_text(encoding="utf-8")
                rel_path = str(py_file.relative_to(self.project_root))

                for pattern, category in patterns:
                    for match in re.finditer(pattern, content):
                        text = match.group(1)
                        if has_translatable_content(text):
                            line_no = content[:match.start()].count('\n') + 1
                            strings.append(ExtractedString(
                                text=text,
                                category=category.value,
                                file=rel_path,
                                line=line_no,
                                context="regex extraction"
                            ))
            except Exception:
                continue

        return strings

    def generate_report(self, strings: List[ExtractedString]) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç –æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏.

        Returns:
            Dict —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∏ –¥–µ—Ç–∞–ª—è–º–∏
        """
        by_category: Dict[str, int] = {}
        by_language: Dict[str, int] = {}
        by_file: Dict[str, int] = {}

        for s in strings:
            by_category[s.category] = by_category.get(s.category, 0) + 1
            by_language[s.language_detected] = by_language.get(s.language_detected, 0) + 1
            by_file[s.file] = by_file.get(s.file, 0) + 1

        return {
            "total_strings": len(strings),
            "by_category": by_category,
            "by_language": by_language,
            "by_file": by_file,
            "with_placeholders": sum(1 for s in strings if s.has_placeholders),
        }

    def export_strings(self, strings: List[ExtractedString], output_path: Path):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ JSON."""
        data = {
            "meta": {
                "project": str(self.project_root),
                "total": len(strings),
                "report": self.generate_report(strings),
            },
            "strings": [asdict(s) for s in strings]
        }
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"  –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(strings)} —Å—Ç—Ä–æ–∫ -> {output_path}")


if __name__ == "__main__":
    import sys

    root = Path(sys.argv[1]) if len(sys.argv) > 1 else Path.cwd()
    scanner = ProjectScanner(root)

    print(f"\n–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞: {root}\n")
    strings = scanner.scan()

    report = scanner.generate_report(strings)

    print(f"\n{'='*60}")
    print(f"–ù–∞–π–¥–µ–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º—ã—Ö —Å—Ç—Ä–æ–∫: {report['total_strings']}")
    print(f"{'='*60}")
    print(f"\n–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for cat, count in sorted(report['by_category'].items()):
        print(f"  {cat}: {count}")
    print(f"\n–ü–æ —è–∑—ã–∫–∞–º:")
    for lang, count in sorted(report['by_language'].items()):
        print(f"  {lang}: {count}")
    print(f"\n–ü–æ —Ñ–∞–π–ª–∞–º:")
    for f, count in sorted(report['by_file'].items(), key=lambda x: -x[1]):
        print(f"  {f}: {count}")

    # –≠–∫—Å–ø–æ—Ä—Ç
    out = root / "src" / "i18n" / "extracted_strings.json"
    scanner.export_strings(strings, out)
