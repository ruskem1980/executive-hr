#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤ i18n –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö (Markdown, HTML, CSV).
"""
import json
import os
import argparse
import csv
from datetime import datetime
from typing import Dict, Any

class I18nReportGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤ i18n –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö.
    """

    def __init__(self, json_report_path: str):
        with open(json_report_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

    def generate_markdown(self, output_path: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –æ—Ç—á–µ—Ç–∞"""
        summary = self.data["summary"]
        stats = self.data["statistics"]
        issues = self.data["issues"]
        coverage = self.data["component_coverage"]

        lines = []
        lines.append("# –û—Ç—á–µ—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ (i18n)\n")
        lines.append(f"**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {self.data['timestamp']}\n")

        lines.append("## üìä –°–≤–æ–¥–∫–∞\n")
        lines.append(f"- **–ë–∞–∑–æ–≤—ã–π —è–∑—ã–∫:** {summary.get('base_lang', 'ru')}\n")
        lines.append(f"- **–í—Å–µ–≥–æ –∫–ª—é—á–µ–π –≤ –±–∞–∑–µ:** {summary['total_keys_base']}\n")
        lines.append(f"- **–Ø–∑—ã–∫–∏:** {', '.join(summary['languages'])}\n")
        lines.append(f"- **–í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º:** {summary['total_issues']} (–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö: {summary['critical_issues']})\n")

        lines.append("\n## üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º\n")
        lines.append("| –Ø–∑—ã–∫ | –ö–ª—é—á–∏ | –°–ª–æ–≤–∞ | –°–∏–º–≤–æ–ª—ã | –ü–æ–∫—Ä—ã—Ç–∏–µ |\n")
        lines.append("|------|-------|-------|----------|----------|\n")
        for lang, s in stats.items():
            lines.append(f"| {lang} | {s['keys_count']} | {s['word_count']} | {s['char_count']} | {s['coverage_percent']:.1f}% |\n")

        lines.append("\n## üõ† –ü–æ–∫—Ä—ã—Ç–∏–µ –∫–æ–¥–∞\n")
        lines.append(f"- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª—é—á–µ–π –≤ –∫–æ–¥–µ: {coverage['used_keys_count']}\n")
        lines.append(f"- –ù–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–ª—é—á–µ–π –≤ JSON: {coverage['unused_keys_count']}\n")
        lines.append(f"- –ö–ª—é—á–µ–π –≤ –∫–æ–¥–µ, –Ω–æ –Ω–µ—Ç –≤ JSON: {coverage['missing_keys_in_code_count']}\n")
        lines.append(f"- –•–∞—Ä–¥–∫–æ–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –Ω–∞–π–¥–µ–Ω–æ: {coverage['hardcoded_strings_count']}\n")

        if coverage.get('hardcoded_samples'):
            lines.append("\n### –ü—Ä–∏–º–µ—Ä—ã —Ö–∞—Ä–¥–∫–æ–¥–∞:\n")
            lines.append("| –§–∞–π–ª | –¢–µ–∫—Å—Ç |\n")
            lines.append("|------|-------|\n")
            for item in coverage['hardcoded_samples']:
                lines.append(f"| `{item['file']}` | {item['text']} |\n")

        lines.append("\n## ‚ö†Ô∏è –°–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º\n")
        lines.append("| –¢–∏–ø | –í–∞–∂–Ω–æ—Å—Ç—å | –Ø–∑—ã–∫ | –ö–ª—é—á | –û–ø–∏—Å–∞–Ω–∏–µ |\n")
        lines.append("|-----|----------|------|------|----------|\n")
        for issue in issues:
            lines.append(f"| {issue['type']} | {issue['severity']} | {issue['lang']} | `{issue['key']}` | {issue['description']} |\n")

        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)

    def generate_csv(self, output_path: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è CSV –æ—Ç—á–µ—Ç–∞"""
        issues = self.data["issues"]
        if not issues:
            return

        keys = issues[0].keys()
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(issues)

    def generate_html(self, output_path: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –æ—Ç—á–µ—Ç–∞"""
        lines = []
        lines.append("<html>\n")
        lines.append("<head>\n")
        lines.append("    <title>i18n Report</title>\n")
        lines.append("    <style>\n")
        lines.append("        body { font-family: sans-serif; margin: 40px; line-height: 1.6; }\n")
        lines.append("        table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }\n")
        lines.append("        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n")
        lines.append("        th { background-color: #f2f2f2; }\n")
        lines.append("        .error { color: red; }\n")
        lines.append("        .warning { color: orange; }\n")
        lines.append("    </style>\n")
        lines.append("</head>\n")
        lines.append("<body>\n")
        lines.append("    <h1>–û—Ç—á–µ—Ç i18n</h1>\n")
        lines.append(f"    <p>–î–∞—Ç–∞: {self.data['timestamp']}</p>\n")
        lines.append("    <h2>–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞</h2>\n")
        lines.append("    <table>\n")
        lines.append("        <tr><th>–Ø–∑—ã–∫</th><th>–ö–ª—é—á–∏</th><th>–°–ª–æ–≤–∞</th><th>–°–∏–º–≤–æ–ª—ã</th><th>–ü–æ–∫—Ä—ã—Ç–∏–µ</th></tr>\n")

        for lang, s in self.data["statistics"].items():
            lines.append(f"        <tr><td>{lang}</td><td>{s['keys_count']}</td><td>{s['word_count']}</td><td>{s['char_count']}</td><td>{s['coverage_percent']:.1f}%</td></tr>\n")

        lines.append("    </table>\n")
        lines.append("    <h2>–ü—Ä–æ–±–ª–µ–º—ã</h2>\n")
        lines.append("    <table>\n")
        lines.append("        <tr><th>–¢–∏–ø</th><th>–í–∞–∂–Ω–æ—Å—Ç—å</th><th>–Ø–∑—ã–∫</th><th>–ö–ª—é—á</th><th>–û–ø–∏—Å–∞–Ω–∏–µ</th></tr>\n")

        for issue in self.data["issues"]:
            severity_class = "error" if issue['severity'] == 'error' else "warning"
            lines.append(f"        <tr><td>{issue['type']}</td><td class='{severity_class}'>{issue['severity']}</td><td>{issue['lang']}</td><td>{issue['key']}</td><td>{issue['description']}</td></tr>\n")

        lines.append("    </table>\n")
        lines.append("</body>\n")
        lines.append("</html>\n")

        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(lines)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤ i18n")
    parser.add_argument("report", help="–ü—É—Ç—å –∫ JSON –æ—Ç—á–µ—Ç—É –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")
    parser.add_argument("--format", choices=["markdown", "csv", "html", "all"], default="all", help="–§–æ—Ä–º–∞—Ç –æ—Ç—á–µ—Ç–∞")
    parser.add_argument("--output-dir", default="reports/i18n", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞")

    args = parser.parse_args()

    gen = I18nReportGenerator(args.report)
    base_name = os.path.basename(args.report).replace('.json', '')

    os.makedirs(args.output_dir, exist_ok=True)

    if args.format in ["markdown", "all"]:
        gen.generate_markdown(os.path.join(args.output_dir, f"{base_name}.md"))
    if args.format in ["csv", "all"]:
        gen.generate_csv(os.path.join(args.output_dir, f"{base_name}.csv"))
    if args.format in ["html", "all"]:
        gen.generate_html(os.path.join(args.output_dir, f"{base_name}.html"))

    print(f"–û—Ç—á–µ—Ç—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ {args.output_dir}")
