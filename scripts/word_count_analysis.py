#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ –ø–µ—Ä–µ–≤–æ–¥–∞—Ö

–§—É–Ω–∫—Ü–∏–∏:
1. –ü–æ–¥—Å—á—ë—Ç —Å–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º —è–∑—ã–∫–µ
2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏ (—Ä—É—Å—Å–∫–∏–π –∫–∞–∫ —ç—Ç–∞–ª–æ–Ω)
3. –í—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π (–≥–¥–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è)
4. –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è–º

–ö—Ä–∏—Ç–µ—Ä–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π:
- –†–∞–∑–Ω–∏—Ü–∞ >50% —Å–ª–æ–≤ —Å—á–∏—Ç–∞–µ—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–π
- –†–∞–∑–Ω–∏—Ü–∞ >100% –∫—Ä–∏—Ç–∏—á–Ω–æ–π
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass
from collections import defaultdict
import sys


@dataclass
class WordCountIssue:
    key: str
    ru_text: str
    ru_words: int
    lang_text: str
    lang_words: int
    diff_percent: float
    severity: str  # "significant" (50-100%) or "critical" (>100%)


class WordCountAnalyzer:
    def __init__(self, locales_dir: Path, reference_lang: str = "ru"):
        self.locales_dir = locales_dir
        self.reference_lang = reference_lang
        self.translations = {}
        self.issues = defaultdict(list)  # lang -> [WordCountIssue]

    def load_translations(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ JSON —Ñ–∞–π–ª—ã"""
        for file in self.locales_dir.glob("*.json"):
            lang = file.stem
            with open(file, 'r', encoding='utf-8') as f:
                self.translations[lang] = json.load(f)

    def flatten_dict(self, d: Dict, parent_key: str = '') -> Dict[str, str]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ –ø–ª–æ—Å–∫–∏–π dict"""
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}.{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(self.flatten_dict(v, new_key).items())
            else:
                items.append((new_key, str(v) if v is not None else ""))
        return dict(items)

    def count_words(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤

        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
        - –ö–∏—Ä–∏–ª–ª–∏—Ü—É (—Ä—É—Å—Å–∫–∏–π, –∫–∏—Ä–≥–∏–∑—Å–∫–∏–π, —Ç–∞–¥–∂–∏–∫—Å–∫–∏–π)
        - –õ–∞—Ç–∏–Ω–∏—Ü—É (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π, —É–∑–±–µ–∫—Å–∫–∏–π)
        - –ü–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã {{var}} –Ω–µ —Å—á–∏—Ç–∞—é—Ç—Å—è
        """
        # –£–±—Ä–∞—Ç—å –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã
        text_clean = re.sub(r'\{\{[^}]+\}\}', '', text)

        # –£–±—Ä–∞—Ç—å –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –∏ –ª–∞—Ç–∏–Ω–∏—Ü—ã
        words = re.findall(r'[\w\u0400-\u04FF]+', text_clean, re.UNICODE)

        return len(words)

    def analyze_word_counts(self):
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤"""
        ref_flat = self.flatten_dict(self.translations[self.reference_lang])

        for lang in self.translations:
            if lang == self.reference_lang:
                continue

            lang_flat = self.flatten_dict(self.translations[lang])

            for key in ref_flat:
                if key not in lang_flat:
                    continue

                ru_text = ref_flat[key]
                lang_text = lang_flat[key]

                ru_words = self.count_words(ru_text)
                lang_words = self.count_words(lang_text)

                # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –µ—Å–ª–∏ –æ–±–∞ —Ç–µ–∫—Å—Ç–∞ –ø—É—Å—Ç—ã–µ –∏–ª–∏ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ
                if ru_words < 2:
                    continue

                # –í—ã—á–∏—Å–ª–∏—Ç—å —Ä–∞–∑–Ω–∏—Ü—É –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
                diff_percent = abs(lang_words - ru_words) / ru_words * 100

                # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å severity
                severity = None
                if diff_percent > 100:
                    severity = "critical"
                elif diff_percent > 50:
                    severity = "significant"

                if severity:
                    self.issues[lang].append(WordCountIssue(
                        key=key,
                        ru_text=ru_text,
                        ru_words=ru_words,
                        lang_text=lang_text,
                        lang_words=lang_words,
                        diff_percent=diff_percent,
                        severity=severity
                    ))

    def print_report(self):
        """–í—ã–≤–µ—Å—Ç–∏ –æ—Ç—á—ë—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É"""
        print("=" * 80)
        print("–ê–ù–ê–õ–ò–ó –ö–û–õ–ò–ß–ï–°–¢–í–ê –°–õ–û–í –í –ü–ï–†–ï–í–û–î–ê–•")
        print("=" * 80)
        print()

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_issues = sum(len(issues) for issues in self.issues.values())
        critical_issues = sum(
            len([i for i in issues if i.severity == "critical"])
            for issues in self.issues.values()
        )
        significant_issues = sum(
            len([i for i in issues if i.severity == "significant"])
            for issues in self.issues.values()
        )

        print(f"–í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º: {total_issues}")
        print(f"  - –ö—Ä–∏—Ç–∏—á–Ω—ã—Ö (>100% —Ä–∞–∑–Ω–∏—Ü–∞): {critical_issues}")
        print(f"  - –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö (50-100% —Ä–∞–∑–Ω–∏—Ü–∞): {significant_issues}")
        print()

        if not total_issues:
            print("‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ –ø–µ—Ä–µ–≤–æ–¥–∞—Ö —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —ç—Ç–∞–ª–æ–Ω—É!")
            print()
            return

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —è–∑—ã–∫–∞–º
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –Ø–ó–´–ö–ê–ú:")
        print("-" * 80)
        for lang in sorted(self.issues.keys()):
            issues = self.issues[lang]
            critical = len([i for i in issues if i.severity == "critical"])
            significant = len([i for i in issues if i.severity == "significant"])

            status = "‚ö†Ô∏è" if critical > 0 else "‚ö°"
            print(f"{status} {lang}: {len(issues)} –ø—Ä–æ–±–ª–µ–º | {critical} –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö | {significant} –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö")
        print()

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –∫–∞–∂–¥–æ–º—É —è–∑—ã–∫—É
        for lang in sorted(self.issues.keys()):
            issues = self.issues[lang]
            if not issues:
                continue

            print(f"{'=' * 80}")
            print(f"–Ø–ó–´–ö: {lang.upper()}")
            print(f"{'=' * 80}")
            print()

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Å–≤–µ—Ä—Ö—É
            issues_sorted = sorted(
                issues,
                key=lambda x: (0 if x.severity == "critical" else 1, -x.diff_percent)
            )

            # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 20
            for i, issue in enumerate(issues_sorted[:20], 1):
                severity_icon = "üî¥" if issue.severity == "critical" else "üü°"
                print(f"{i}. {severity_icon} {issue.key}")
                print(f"   –†–∞–∑–Ω–∏—Ü–∞: {issue.diff_percent:.1f}%")
                print(f"   –†–£ ({issue.ru_words} —Å–ª–æ–≤): {issue.ru_text[:80]}{'...' if len(issue.ru_text) > 80 else ''}")
                print(f"   {lang.upper()} ({issue.lang_words} —Å–ª–æ–≤): {issue.lang_text[:80]}{'...' if len(issue.lang_text) > 80 else ''}")
                print()

            if len(issues_sorted) > 20:
                print(f"... –∏ –µ—â—ë {len(issues_sorted) - 20} –ø—Ä–æ–±–ª–µ–º")
                print()

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("=" * 80)
        print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
        print("=" * 80)
        print()
        print("1. –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã (>100% —Ä–∞–∑–Ω–∏—Ü–∞) —Ç—Ä–µ–±—É—é—Ç –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –ø–µ—Ä–µ–≤–æ–¥–∞")
        print("2. –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã (50-100%) –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å")
        print("3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ scripts/fix_translations.py –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        print("4. –ò–ª–∏ –∏—Å–ø—Ä–∞–≤—å—Ç–µ –≤—Ä—É—á–Ω—É—é –≤ apps/frontend/src/locales/<lang>.json")
        print()

    def export_json(self, output_file: Path):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        export_data = {
            "summary": {
                "total_issues": sum(len(issues) for issues in self.issues.values()),
                "critical": sum(
                    len([i for i in issues if i.severity == "critical"])
                    for issues in self.issues.values()
                ),
                "significant": sum(
                    len([i for i in issues if i.severity == "significant"])
                    for issues in self.issues.values()
                )
            },
            "by_language": {}
        }

        for lang, issues in self.issues.items():
            export_data["by_language"][lang] = [
                {
                    "key": issue.key,
                    "ru_text": issue.ru_text,
                    "ru_words": issue.ru_words,
                    "lang_text": issue.lang_text,
                    "lang_words": issue.lang_words,
                    "diff_percent": round(issue.diff_percent, 2),
                    "severity": issue.severity
                }
                for issue in issues
            ]

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤: {output_file}")
        print()


def main():
    locales_dir = Path(__file__).parent.parent / "apps" / "frontend" / "src" / "locales"

    if not locales_dir.exists():
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {locales_dir}")
        return 1

    analyzer = WordCountAnalyzer(locales_dir)

    print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–æ–≤...")
    analyzer.load_translations()

    print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —è–∑—ã–∫–æ–≤: {', '.join(sorted(analyzer.translations.keys()))}")
    print(f"üìè –≠—Ç–∞–ª–æ–Ω–Ω—ã–π —è–∑—ã–∫: {analyzer.reference_lang}")
    print()

    print("‚öôÔ∏è  –ê–Ω–∞–ª–∏–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤...")
    analyzer.analyze_word_counts()

    analyzer.print_report()

    # –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON
    output_file = Path(__file__).parent.parent / "data" / "word_count_analysis.json"
    output_file.parent.mkdir(parents=True, exist_ok=True)
    analyzer.export_json(output_file)

    # –ö–æ–¥ –≤–æ–∑–≤—Ä–∞—Ç–∞
    if analyzer.issues:
        return 1  # –ï—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
    else:
        return 0  # –í—Å—ë –æ–∫


if __name__ == "__main__":
    sys.exit(main())
