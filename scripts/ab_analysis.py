#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ML vs Rules –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

–ü–∞—Ä—Å–∏—Ç ab_test_log.jsonl –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
- –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å ML –∏ rules
- Confidence –ø–æ –≥—Ä—É–ø–ø–∞–º
- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ fallback-–æ–≤

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 scripts/ab_analysis.py                    # –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç
    python3 scripts/ab_analysis.py --json              # JSON —Ñ–æ—Ä–º–∞—Ç
    python3 scripts/ab_analysis.py --since 2026-02-13  # –° –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –¥–∞—Ç—ã
"""

import sys
import os
import json
import argparse
from datetime import datetime
from collections import Counter, defaultdict

ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
AB_LOG_PATH = os.path.join(ROOT, '.claude', 'tracking', 'ab_test_log.jsonl')


def load_ab_data(since: str = None) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    if not os.path.exists(AB_LOG_PATH):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {AB_LOG_PATH}")
        return []

    entries = []
    with open(AB_LOG_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                entry = json.loads(line)
                if since and entry.get('timestamp', '') < since:
                    continue
                entries.append(entry)
            except json.JSONDecodeError:
                continue

    return entries


def analyze(entries: list) -> dict:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ A/B –¥–∞–Ω–Ω—ã—Ö."""
    if not entries:
        return {'error': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞'}

    # –ë–∞–∑–æ–≤—ã–µ –ø–æ–¥—Å—á—ë—Ç—ã
    total = len(entries)
    ab_groups = Counter(e.get('abGroup') for e in entries)
    methods = Counter(e.get('classificationMethod') for e in entries)
    final_levels = Counter(e.get('finalLevel') for e in entries)
    rules_levels = Counter(e.get('rulesLevel') for e in entries)

    # ML-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    ml_entries = [e for e in entries if e.get('mlLevel') is not None]
    ml_methods = Counter(e.get('mlMethod') for e in ml_entries)

    # –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å ML –∏ rules
    agreement_count = 0
    disagreement_details = []
    for e in ml_entries:
        if e.get('mlLevel') == e.get('rulesLevel'):
            agreement_count += 1
        else:
            disagreement_details.append({
                'task': e.get('task', '')[:60],
                'rules': e.get('rulesLevel'),
                'ml': e.get('mlLevel'),
                'final': e.get('finalLevel'),
                'ml_conf': e.get('mlConfidence'),
                'rules_conf': e.get('rulesConfidence')
            })

    agreement_rate = agreement_count / len(ml_entries) if ml_entries else 0

    # Confidence —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    rules_confidences = [e.get('rulesConfidence', 0) for e in entries if e.get('rulesConfidence')]
    ml_confidences = [e.get('mlConfidence', 0) for e in ml_entries if e.get('mlConfidence')]

    avg_rules_conf = sum(rules_confidences) / len(rules_confidences) if rules_confidences else 0
    avg_ml_conf = sum(ml_confidences) / len(ml_confidences) if ml_confidences else 0

    # ML fallback –∞–Ω–∞–ª–∏–∑
    ml_direct = sum(1 for e in ml_entries if e.get('mlMethod') == 'ml')
    ml_fallback = sum(1 for e in ml_entries
                      if e.get('mlMethod', '').startswith('fallback'))
    ml_direct_rate = ml_direct / len(ml_entries) if ml_entries else 0

    # –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞
    timestamps = sorted(set(e.get('timestamp', '')[:10] for e in entries))

    return {
        'summary': {
            'total_entries': total,
            'unique_tasks': len(set(e.get('task', '') for e in entries)),
            'date_range': {
                'from': timestamps[0] if timestamps else None,
                'to': timestamps[-1] if timestamps else None,
            }
        },
        'ab_groups': dict(ab_groups),
        'classification_methods': dict(methods),
        'final_levels': dict(final_levels),
        'rules_levels': dict(rules_levels),
        'ml_analysis': {
            'total_ml_calls': len(ml_entries),
            'ml_methods': dict(ml_methods),
            'ml_direct_rate': round(ml_direct_rate, 3),
            'ml_fallback_count': ml_fallback,
            'agreement_with_rules': round(agreement_rate, 3),
            'disagreements': disagreement_details[:10],  # –¢–æ–ø-10
        },
        'confidence': {
            'rules_avg': round(avg_rules_conf, 3),
            'ml_avg': round(avg_ml_conf, 3),
            'rules_min': round(min(rules_confidences), 3) if rules_confidences else 0,
            'rules_max': round(max(rules_confidences), 3) if rules_confidences else 0,
            'ml_min': round(min(ml_confidences), 3) if ml_confidences else 0,
            'ml_max': round(max(ml_confidences), 3) if ml_confidences else 0,
        },
        'recommendations': generate_recommendations(
            agreement_rate, avg_ml_conf, ml_direct_rate,
            len(ml_entries), total
        )
    }


def generate_recommendations(agreement: float, ml_conf: float,
                              ml_direct: float, ml_count: int,
                              total: int) -> list:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    recs = []

    if ml_count < 20:
        recs.append("‚ö†Ô∏è  –ú–∞–ª–æ ML-–¥–∞–Ω–Ω—ã—Ö ({} –∑–∞–ø–∏—Å–µ–π). –£–≤–µ–ª–∏—á—å—Ç–µ mlRatio –∏–ª–∏ –ø–æ–¥–æ–∂–¥–∏—Ç–µ –±–æ–ª—å—à–µ –∑–∞–¥–∞—á.".format(ml_count))

    if ml_conf < 0.7 and ml_count > 0:
        recs.append("üìâ –°—Ä–µ–¥–Ω–∏–π ML confidence ({:.1%}) –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ 70%. –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —Ç—é–Ω–∏–Ω–≥ –º–æ–¥–µ–ª–∏.".format(ml_conf))

    if ml_direct < 0.3 and ml_count > 10:
        recs.append("üîÑ ML –Ω–∞–ø—Ä—è–º—É—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ {:.0%} —Å–ª—É—á–∞–µ–≤. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π ‚Äî fallback –Ω–∞ rules.".format(ml_direct))

    if agreement > 0.9 and ml_count > 20:
        recs.append("‚úÖ ML –∏ rules —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã –≤ {:.0%} —Å–ª—É—á–∞–µ–≤. –ú–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å mlRatio –¥–æ 0.7-0.8.".format(agreement))
    elif agreement < 0.5 and ml_count > 10:
        recs.append("‚ö†Ô∏è  –ù–∏–∑–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å ML/rules ({:.0%}). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.".format(agreement))

    if not recs:
        recs.append("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö.")

    return recs


def print_report(analysis: dict):
    """–ü–µ—á–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞."""
    if 'error' in analysis:
        print(f"‚ùå {analysis['error']}")
        return

    s = analysis['summary']
    print("=" * 70)
    print("üìä –ê–ù–ê–õ–ò–ó A/B –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø: ML vs RULES")
    print("=" * 70)
    print(f"  –ü–µ—Ä–∏–æ–¥: {s['date_range']['from']} ‚Äî {s['date_range']['to']}")
    print(f"  –ó–∞–ø–∏—Å–µ–π: {s['total_entries']}, —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á: {s['unique_tasks']}")

    print("\n--- A/B –ì—Ä—É–ø–ø—ã ---")
    for group, count in analysis['ab_groups'].items():
        pct = count / s['total_entries'] * 100
        print(f"  {group:>10}: {count:>4} ({pct:.0f}%)")

    print("\n--- –ú–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ---")
    for method, count in analysis['classification_methods'].items():
        pct = count / s['total_entries'] * 100
        print(f"  {method:>25}: {count:>4} ({pct:.0f}%)")

    print("\n--- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (—Ñ–∏–Ω–∞–ª—å–Ω–æ–µ) ---")
    for level in ['program', 'simple', 'medium', 'complex']:
        count = analysis['final_levels'].get(level, 0)
        pct = count / s['total_entries'] * 100
        bar = "‚ñà" * int(pct / 2)
        print(f"  {level:>10}: {count:>4} ({pct:>5.1f}%) {bar}")

    ml = analysis['ml_analysis']
    print(f"\n--- ML –ê–Ω–∞–ª–∏–∑ ({ml['total_ml_calls']} –≤—ã–∑–æ–≤–æ–≤) ---")
    if ml['total_ml_calls'] > 0:
        print(f"  ML –Ω–∞–ø—Ä—è–º—É—é:        {ml['ml_direct_rate']:.0%}")
        print(f"  ML fallback:        {ml['ml_fallback_count']} —Ä–∞–∑")
        print(f"  –°–æ–≥–ª–∞—Å–∏–µ —Å rules:   {ml['agreement_with_rules']:.0%}")
        print(f"  ML –º–µ—Ç–æ–¥—ã:          {ml['ml_methods']}")

        if ml['disagreements']:
            print(f"\n  –†–∞–∑–Ω–æ–≥–ª–∞—Å–∏—è ML/rules (—Ç–æ–ø-{len(ml['disagreements'])}):")
            for d in ml['disagreements'][:5]:
                print(f"    ¬´{d['task']}¬ª")
                print(f"      rules={d['rules']} vs ml={d['ml']} ‚Üí final={d['final']}"
                      f" (ml_conf={d['ml_conf']:.2f})")
    else:
        print("  –ù–µ—Ç ML –≤—ã–∑–æ–≤–æ–≤ –≤ –ª–æ–≥–µ.")

    c = analysis['confidence']
    print(f"\n--- Confidence ---")
    print(f"  Rules:  avg={c['rules_avg']:.2f}  min={c['rules_min']:.2f}  max={c['rules_max']:.2f}")
    if c['ml_avg'] > 0:
        print(f"  ML:     avg={c['ml_avg']:.2f}  min={c['ml_min']:.2f}  max={c['ml_max']:.2f}")

    print(f"\n--- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ---")
    for rec in analysis['recommendations']:
        print(f"  {rec}")

    print("=" * 70)


def main():
    parser = argparse.ArgumentParser(description='–ê–Ω–∞–ª–∏–∑ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ML vs Rules')
    parser.add_argument('--json', action='store_true', help='–í—ã–≤–æ–¥ –≤ JSON')
    parser.add_argument('--since', type=str, help='–ê–Ω–∞–ª–∏–∑ —Å –¥–∞—Ç—ã (YYYY-MM-DD)')

    args = parser.parse_args()

    entries = load_ab_data(since=args.since)
    analysis = analyze(entries)

    if args.json:
        print(json.dumps(analysis, indent=2, ensure_ascii=False))
    else:
        print_report(analysis)


if __name__ == '__main__':
    main()
