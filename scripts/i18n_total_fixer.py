#!/usr/bin/env python3
"""
Comprehensive i18n scanner –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π fixer.
–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∑–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö.
"""
import os
import re
import json
import shutil
import difflib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass, field

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
PROJECT_ROOT = Path.cwd()
SRC_DIR = PROJECT_ROOT / "apps/frontend/src"
COMPONENTS_DIR = SRC_DIR / "components"
LOCALES_DIR = SRC_DIR / "locales"
LANGUAGES = ["ru", "en", "uz", "tg", "ky"]
BACKUP_DIR = PROJECT_ROOT / "i18n_backups" / datetime.now().strftime("%Y%m%d_%H%M%S")
PATCHES_DIR = PROJECT_ROOT / "i18n_patches"

# –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
CYRILLIC_REGEX = r'[\u0400-\u04FF]+'
# 1. –¢–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ JSX: <div>–¢–µ–∫—Å—Ç</div>
JSX_TEXT_PATTERN = re.compile(r'>\s*([^<{]*[\u0400-\u04FF][^<{]*?)\s*<')
# 2. –°—Ç—Ä–æ–∫–æ–≤—ã–µ –ª–∏—Ç–µ—Ä–∞–ª—ã: '–¢–µ–∫—Å—Ç', "–¢–µ–∫—Å—Ç", `–¢–µ–∫—Å—Ç` (–Ω–æ –Ω–µ –∫–ª—é—á–∏ –æ–±—ä–µ–∫—Ç–æ–≤)
STRING_LITERAL_PATTERN = re.compile(r'(?<![a-zA-Z0-9_])["\'`]([\u0400-\u04FF][^"\'`]*?)["\'`](?!\s*[:])')
# 3. Props —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏: label="–¢–µ–∫—Å—Ç"
PROP_VALUE_PATTERN = re.compile(r'(title|label|placeholder|aria-label|description|text)\s*=\s*["\']([\u0400-\u04FF][^"\']+)["\']')

@dataclass
class Finding:
    file_path: Path
    line_number: int
    original_text: str
    context_type: str  # 'jsx', 'string', 'prop'
    suggested_key: str = ""
    full_line: str = ""

class I18nFixer:
    def __init__(self):
        self.findings: List[Finding] = []
        self.translations: Dict[str, Dict[str, str]] = {lang: self._load_locale(lang) for lang in LANGUAGES}
        self.new_keys_count = 0

    def _load_locale(self, lang: str) -> Dict:
        path = LOCALES_DIR / f"{lang}.json"
        if path.exists():
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def _save_locales(self):
        BACKUP_DIR.mkdir(parents=True, exist_ok=True)
        for lang, data in self.translations.items():
            path = LOCALES_DIR / f"{lang}.json"
            # –î–µ–ª–∞–µ–º –±—ç–∫–∞–ø
            if path.exists():
                shutil.copy(path, BACKUP_DIR / f"{lang}.json.bak")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π –∏ –æ—Ç—Å—Ç—É–ø–∞–º–∏
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, sort_keys=True)

    def slugify(self, text: str) -> str:
        """–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∞—Å—Ç–∏ –∫–ª—é—á–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        text = text.lower()
        # –û—á–µ–Ω—å —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤
        text = re.sub(r'[^–∞-—èa-z0-9\s]', '', text)
        words = text.split()[:3] # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 —Å–ª–æ–≤–∞
        return "_".join(words)

    def generate_key(self, finding: Finding) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á —Ç–∏–ø–∞ 'paywall.sheet.select_plan'"""
        rel_path = finding.file_path.relative_to(SRC_DIR)
        parts = list(rel_path.parent.parts)
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
        filename = finding.file_path.stem
        if filename not in parts:
            parts.append(filename)

        # –û—á–∏—â–∞–µ–º —á–∞—Å—Ç–∏ –ø—É—Ç–∏
        section = parts[1] if len(parts) > 1 else "common"
        subsection = parts[-1].lower()

        # –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–æ–ø—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∏–º—è –≤ –∫–ª—é—á–µ
        if finding.context_type == 'prop':
            match = PROP_VALUE_PATTERN.search(finding.full_line)
            if match:
                prop_name = match.group(1)
                return f"{section}.{subsection}.{prop_name}"

        suffix = self.slugify(finding.original_text)
        return f"{section}.{subsection}.{suffix}"

    def scan(self):
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã, —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –¥–∞–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ñ–∏–≥–∏ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ö–∞—Ä–¥–∫–æ–¥–∞"""
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: components, app, data, config, lib
        scan_dirs = [
            COMPONENTS_DIR,
            SRC_DIR / "app",
            SRC_DIR / "data",
            SRC_DIR / "config",
            SRC_DIR / "lib"
        ]

        files = []
        for scan_dir in scan_dirs:
            if scan_dir.exists():
                files.extend(scan_dir.glob("**/*.tsx"))
                files.extend(scan_dir.glob("**/*.ts"))

        print(f"  –ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")

        for file_path in files:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            for i, line in enumerate(lines):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–º–ø–æ—Ä—Ç—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
                if line.strip().startswith(('import', '//', '*', '/*')):
                    continue

                # –ü–æ–∏—Å–∫ –≤ JSX —Ç–µ–∫—Å—Ç–∞—Ö
                for match in JSX_TEXT_PATTERN.finditer(line):
                    text = match.group(1).strip()
                    if text:
                        self.findings.append(Finding(file_path, i+1, text, 'jsx', full_line=line))

                # –ü–æ–∏—Å–∫ –≤ –∫–∞–≤—ã—á–∫–∞—Ö (—Å—Ç—Ä–æ–∫–∏)
                for match in STRING_LITERAL_PATTERN.finditer(line):
                    text = match.group(1).strip()
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∫–ª—é—á (—Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ—á–∫—É –∏ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤)
                    if '.' in text and ' ' not in text:
                        continue
                    self.findings.append(Finding(file_path, i+1, text, 'string', full_line=line))

                # –ü–æ–∏—Å–∫ –≤ –ø—Ä–æ–ø—Å–∞—Ö
                for match in PROP_VALUE_PATTERN.finditer(line):
                    prop_name = match.group(1)
                    text = match.group(2).strip()
                    self.findings.append(Finding(file_path, i+1, text, 'prop', full_line=line))

    def update_json_files(self):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –∫–ª—é—á–∏ –≤ JSON"""
        for finding in self.findings:
            key = self.generate_key(finding)
            finding.suggested_key = key

            keys = key.split('.')

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Å–µ —è–∑—ã–∫–∏
            for lang in LANGUAGES:
                current = self.translations[lang]
                for i, k in enumerate(keys[:-1]):
                    if k not in current:
                        current[k] = {}
                    current = current[k]

                last_key = keys[-1]
                if last_key not in current:
                    if lang == 'ru':
                        current[last_key] = finding.original_text
                    else:
                        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤
                        current[last_key] = f"[{lang.upper()}] {finding.original_text}"
                    self.new_keys_count += 1

        self._save_locales()

    def create_patches(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø–∞—Ç—á–∏ –¥–ª—è —Ñ–∞–π–ª–æ–≤"""
        if PATCHES_DIR.exists():
            shutil.rmtree(PATCHES_DIR)
        PATCHES_DIR.mkdir(parents=True)

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–∞—Ö–æ–¥–∫–∏ –ø–æ —Ñ–∞–π–ª–∞–º
        file_map: Dict[Path, List[Finding]] = {}
        for f in self.findings:
            if f.file_path not in file_map:
                file_map[f.file_path] = []
            file_map[f.file_path].append(f)

        for file_path, findings in file_map.items():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            new_content = content

            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º/–î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
            if "useTranslation" not in new_content:
                import_stmt = "import { useTranslation } from '@/lib/i18n';\n"
                new_content = import_stmt + new_content

            # 2. –î–æ–±–∞–≤–ª—è–µ–º —Ö—É–∫ –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–æ—á–µ–Ω—å —É–ø—Ä–æ—â–µ–Ω–Ω–æ - –∏—â–µ–º –ø–µ—Ä–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é)
            if "const { t } = useTranslation" not in new_content:
                # –ò—â–µ–º –Ω–∞—á–∞–ª–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
                new_content = re.sub(
                    r'(export (function|const) \w+.*?[{=(].*?\n)',
                    r'\1  const { t } = useTranslation();\n',
                    new_content,
                    count=1
                )

            # 3. –ó–∞–º–µ–Ω—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ t('key')
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—Ö–æ–¥–∫–∏ –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ (—Å–Ω–∞—á–∞–ª–∞ –¥–ª–∏–Ω–Ω—ã–µ), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã—Ö –∑–∞–º–µ–Ω
            findings.sort(key=lambda x: len(x.original_text), reverse=True)

            processed_texts = set()
            for f in findings:
                if f.original_text in processed_texts: continue

                key = f.suggested_key
                if f.context_type == 'jsx':
                    # <div>–¢–µ–∫—Å—Ç</div> -> <div>{t('key')}</div>
                    new_content = new_content.replace(f">{f.original_text}<", f">{{t('{key}')}}<")
                elif f.context_type == 'prop':
                    # label="–¢–µ–∫—Å—Ç" -> label={t('key')}
                    new_content = re.sub(
                        rf'{f.context_type}="({re.escape(f.original_text)})"',
                        f'{f.context_type}={{t(\'{key}\')}}',
                        new_content
                    )
                else:
                    # '–¢–µ–∫—Å—Ç' -> t('key')
                    new_content = new_content.replace(f"'{f.original_text}'", f"t('{key}')")
                    new_content = new_content.replace(f'"{f.original_text}"', f"t('{key}')")

                processed_texts.add(f.original_text)

            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—á
            diff = difflib.unified_diff(
                content.splitlines(keepends=True),
                new_content.splitlines(keepends=True),
                fromfile=str(file_path),
                tofile=str(file_path)
            )

            patch_name = f"{file_path.stem}.patch"
            with open(PATCHES_DIR / patch_name, 'w', encoding='utf-8') as f:
                f.writelines(diff)

    def print_report(self):
        print("\n" + "‚ïê"*50)
        print("üîç –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print("‚ïê"*50)
        print(f"–§–∞–π–ª–æ–≤ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ:     {len(set(f.file_path for f in self.findings))}")
        print(f"–ù–∞–π–¥–µ–Ω–æ —Ö–∞—Ä–¥–∫–æ–¥–∞:     {len(self.findings)}")
        print(f"–ù–æ–≤—ã—Ö –∫–ª—é—á–µ–π —Å–æ–∑–¥–∞–Ω–æ: {self.new_keys_count}")

        print("\nüìä –¢–û–ü –ü–†–û–ë–õ–ï–ú–ù–´–• –§–ê–ô–õ–û–í:")
        file_counts = {}
        for f in self.findings:
            file_counts[f.file_path.name] = file_counts.get(f.file_path.name, 0) + 1

        sorted_files = sorted(file_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        for name, count in sorted_files:
            print(f"  {name:<30} | {count} —Å—Ç—Ä–æ–∫")

        print("\n‚úÖ –õ–û–ö–ê–õ–ò –û–ë–ù–û–í–õ–ï–ù–´ (Backups in i18n_backups/)")
        print("üîß –ü–ê–¢–ß–ò –°–û–ó–î–ê–ù–´ –≤ i18n_patches/")
        print("\nüí° –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        print("  1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ i18n_patches/*.patch")
        print("  2. –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –∏—Ö: `patch -p0 < i18n_patches/File.patch` –∏–ª–∏ `git apply` –µ—Å–ª–∏ –≤ –∫–æ—Ä–Ω–µ")
        print("  3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç—ã —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞")

    def apply_changes_directly(self):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞–ø—Ä—è–º—É—é –≤ —Ñ–∞–π–ª—ã (–±–µ–∑ –ø–∞—Ç—á–µ–π)"""
        print("  –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞–ø—Ä—è–º—É—é –≤ —Ñ–∞–π–ª—ã...")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –Ω–∞—Ö–æ–¥–∫–∏ –ø–æ —Ñ–∞–π–ª–∞–º
        file_map: Dict[Path, List[Finding]] = {}
        for f in self.findings:
            if f.file_path not in file_map:
                file_map[f.file_path] = []
            file_map[f.file_path].append(f)

        changed_count = 0
        for file_path, findings in file_map.items():
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            original_content = content
            new_content = content

            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º/–î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç
            if "useTranslation" not in new_content and "'use client'" in new_content:
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç –ø–æ—Å–ª–µ 'use client'
                new_content = new_content.replace(
                    "'use client';",
                    "'use client';\n\nimport { useTranslation } from '@/lib/i18n';"
                )

            # 2. –î–æ–±–∞–≤–ª—è–µ–º —Ö—É–∫ –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ)
            if "const { t } = useTranslation" not in new_content and "useTranslation" in new_content:
                # –ò—â–µ–º export function –ò–º—è–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ –∏–ª–∏ export default function
                # –î–æ–±–∞–≤–ª—è–µ–º —Ö—É–∫ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –æ—Ç–∫—Ä—ã–≤–∞—é—â–µ–π —Ñ–∏–≥—É—Ä–Ω–æ–π —Å–∫–æ–±–∫–∏
                function_pattern = r'(export\s+(default\s+)?function\s+\w+[^{]*\{)'
                if re.search(function_pattern, new_content):
                    new_content = re.sub(
                        function_pattern,
                        r'\1\n  const { t } = useTranslation();',
                        new_content,
                        count=1
                    )

            # 3. –ó–∞–º–µ–Ω—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ t('key')
            findings.sort(key=lambda x: len(x.original_text), reverse=True)

            processed_texts = set()
            for f in findings:
                if f.original_text in processed_texts:
                    continue

                key = f.suggested_key
                escaped_text = re.escape(f.original_text)

                # JSX —Ç–µ–∫—Å—Ç
                if f.context_type == 'jsx':
                    new_content = new_content.replace(
                        f">{f.original_text}<",
                        f">{{t('{key}')}}<"
                    )

                # Props
                elif f.context_type == 'prop':
                    new_content = re.sub(
                        rf'(title|label|placeholder|aria-label|description|text)="({escaped_text})"',
                        rf'\1={{t(\'{key}\')}}',
                        new_content
                    )

                # –°—Ç—Ä–æ–∫–æ–≤—ã–µ –ª–∏—Ç–µ—Ä–∞–ª—ã
                else:
                    new_content = new_content.replace(f"'{f.original_text}'", f"t('{key}')")
                    new_content = new_content.replace(f'"{f.original_text}"', f"t('{key}')")

                processed_texts.add(f.original_text)

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±—ã–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            if new_content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                changed_count += 1

        print(f"  ‚úÖ –ò–∑–º–µ–Ω–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {changed_count}")

def main():
    import sys

    print("üöÄ –ó–∞–ø—É—Å–∫ Total i18n Fixer...")
    fixer = I18nFixer()

    print("üìÅ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
    fixer.scan()

    if not fixer.findings:
        print("‚ú® –ß–∏—Å—Ç–æ! –ó–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return

    print(f"üìù –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(fixer.findings)} –Ω–∞—Ö–æ–¥–æ–∫...")
    fixer.update_json_files()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ --apply
    if '--apply' in sys.argv:
        print("üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–∞–ø—Ä—è–º—É—é...")
        fixer.apply_changes_directly()
    else:
        print("üõ† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–∞—Ç—á–µ–π...")
        fixer.create_patches()

    fixer.print_report()

    if '--apply' not in sys.argv:
        print("\nüí° –î–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python3 scripts/i18n_total_fixer.py --apply")

if __name__ == "__main__":
    main()
